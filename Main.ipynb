{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and training Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 1.3.0\n",
      "syft version: 0.2.0a2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import syft as sy\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"syft version:\", sy.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSamples(filename):\n",
    "    data = pd.read_csv(filename, sep='\\t')\n",
    "    return data.to_numpy()[:,1:].transpose()\n",
    "\n",
    "dim = 12634\n",
    "data1 = getSamples(\"GSE2034-Normal-train.txt\")\n",
    "data2 = getSamples(\"GSE2034-Tumor-train.txt\")\n",
    "\n",
    "data1Label = np.zeros(len(data1)).reshape((-1, 1))\n",
    "data2Label = np.ones(len(data2)).reshape((-1, 1))\n",
    "x = np.concatenate((data1, data2))\n",
    "y = np.concatenate((data1Label, data2Label))\n",
    "\n",
    "# shuffle the data\n",
    "idx = np.random.permutation(len(x))\n",
    "x,y = x[idx], y[idx]\n",
    "\n",
    "z = np.concatenate((x, y), axis = 1)\n",
    "\n",
    "# We follow an 80/20 partitioning for the training and testing sets\n",
    "n_train_items = 181\n",
    "n_test_items = 46\n",
    "\n",
    "# partition the data into training data and test data\n",
    "x_train = x[:n_train_items]\n",
    "y_train = y[:n_train_items]\n",
    "\n",
    "x_train = x_train.reshape((-1,1,dim))\n",
    "y_train = y_train.reshape((-1,1))\n",
    "\n",
    "x_test = x[n_train_items:]\n",
    "y_test = y[n_train_items:]    \n",
    "\n",
    "x_test = x_test.reshape((-1,1,dim))\n",
    "y_test = y_test.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data into (batch, channel = 1, length=dim)\n",
    "data_torch = torch.from_numpy(x).view([-1, 1, dim]).float()\n",
    "label_torch = torch.from_numpy(y).view([-1,1,1]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Model applied to GSE2034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 12634\n",
    "\n",
    "class Res1d(nn.Module):\n",
    "    # the conv layers\n",
    "    def __init__(self, inSize, outSize, kernel=(3,), strides=1,):\n",
    "        super(Res1d, self).__init__()\n",
    "        \n",
    "        # Left , kernel size 3\n",
    "        # hard-coded to do the padding correctly\n",
    "        if inSize in (16,64,128,512) and strides > 1:\n",
    "            pding = 0\n",
    "        else:\n",
    "            pding = 1\n",
    "            \n",
    "        self.l = nn.Sequential(\n",
    "            nn.Conv1d(inSize, outSize, kernel, stride=strides, padding=pding, bias=False),\n",
    "            nn.InstanceNorm1d(outSize)\n",
    "        )    \n",
    "        \n",
    "        # Right, kernel size 1\n",
    "        if inSize != outSize or strides > 1:\n",
    "            if strides > 1:\n",
    "                self.r1 = nn.AvgPool1d(strides)\n",
    "            else:\n",
    "                self.r1 = nn.Identity()\n",
    "                \n",
    "        self.r = nn.Sequential(\n",
    "            self.r1,\n",
    "            nn.Conv1d(inSize, outSize, 1, bias=False),\n",
    "            nn.InstanceNorm1d(outSize)\n",
    "        )\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):          \n",
    "        x = self.l(x) + self.r(x)\n",
    "        return self.relu(x)\n",
    "    \n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.l = nn.Sequential(\n",
    "            nn.Linear(dim, 64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.r = nn.Sequential(\n",
    "            Res1d(1, 4, 3),\n",
    "\n",
    "            Res1d(4, 8, 3),\n",
    "            Res1d(8, 8, 3, strides=2),\n",
    "\n",
    "            Res1d(8, 16, 3),\n",
    "            Res1d(16, 16, 3, strides=2),\n",
    "\n",
    "            Res1d(16, 32, 3),\n",
    "            Res1d(32, 32, 3, strides=2),\n",
    "\n",
    "            Res1d(32, 64, 3),\n",
    "            Res1d(64, 64, 3, strides=2),\n",
    "\n",
    "            Res1d(64, 128, 3),\n",
    "            Res1d(128, 128, 3, strides=2),\n",
    "\n",
    "            Res1d(128, 256, 3),\n",
    "            Res1d(256, 256, 3, strides=2),\n",
    "\n",
    "            Res1d(256, 512, 3),\n",
    "            Res1d(512, 512, 3, strides=2),\n",
    "\n",
    "            Res1d(512, 1024, 3),\n",
    "            Res1d(1024, 1024, 3, strides=2),\n",
    "\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # size is by experiment and hardcode\n",
    "        self.last = nn.Sequential(\n",
    "            nn.Linear(50240,32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # shape is (batch, channel, time)\n",
    "        l = self.l(x)\n",
    "        r = self.r(x)\n",
    "        r.unsqueeze_(-2) # add channel dimension\n",
    "        y = torch.cat((l,r),dim=-1)\n",
    "        y = self.last(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "net = Net().to(device)\n",
    "\n",
    "#from torchsummary import summary\n",
    "#summary(net,(1,12634))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy \n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on plain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 % Trained loss =  0.7175427675247192\n",
      "1 % Trained loss =  0.5960087180137634\n",
      "2 % Trained loss =  0.8237826228141785\n",
      "3 % Trained loss =  0.6514562964439392\n",
      "4 % Trained loss =  0.5533632636070251\n",
      "5 % Trained loss =  0.6631456017494202\n",
      "6 % Trained loss =  0.644184947013855\n",
      "7 % Trained loss =  0.6296128630638123\n",
      "8 % Trained loss =  0.5669456720352173\n",
      "9 % Trained loss =  0.5513830780982971\n",
      "10 % Trained loss =  0.5824121832847595\n",
      "11 % Trained loss =  0.5907394289970398\n",
      "12 % Trained loss =  0.4805615544319153\n",
      "13 % Trained loss =  0.47551998496055603\n",
      "14 % Trained loss =  0.43236055970191956\n",
      "15 % Trained loss =  0.442240834236145\n",
      "16 % Trained loss =  0.4249318242073059\n",
      "17 % Trained loss =  0.4000908434391022\n",
      "18 % Trained loss =  0.5188621878623962\n",
      "19 % Trained loss =  0.37506961822509766\n",
      "20 % Trained loss =  0.2837182581424713\n",
      "21 % Trained loss =  0.28202465176582336\n",
      "22 % Trained loss =  0.43621012568473816\n",
      "23 % Trained loss =  0.20410969853401184\n",
      "24 % Trained loss =  0.17711634933948517\n",
      "25 % Trained loss =  0.2257910668849945\n",
      "26 % Trained loss =  0.3207176923751831\n",
      "27 % Trained loss =  0.21483033895492554\n",
      "28 % Trained loss =  0.216986745595932\n",
      "29 % Trained loss =  0.1815575659275055\n",
      "30 % Trained loss =  0.12094931304454803\n",
      "31 % Trained loss =  0.10572394728660583\n",
      "32 % Trained loss =  0.12388192117214203\n",
      "33 % Trained loss =  0.12320542335510254\n",
      "34 % Trained loss =  0.08117971569299698\n",
      "35 % Trained loss =  0.09517260640859604\n",
      "36 % Trained loss =  0.08774053305387497\n",
      "37 % Trained loss =  0.06098630279302597\n",
      "38 % Trained loss =  0.05239111930131912\n",
      "39 % Trained loss =  0.07722511142492294\n",
      "40 % Trained loss =  0.05176885798573494\n",
      "41 % Trained loss =  0.044649939984083176\n",
      "42 % Trained loss =  0.0663352832198143\n",
      "43 % Trained loss =  0.04893394932150841\n",
      "44 % Trained loss =  0.03173600137233734\n",
      "45 % Trained loss =  0.0342254601418972\n",
      "46 % Trained loss =  0.040939975529909134\n",
      "47 % Trained loss =  0.037572864443063736\n",
      "48 % Trained loss =  0.03105679713189602\n",
      "49 % Trained loss =  0.021703733131289482\n",
      "50 % Trained loss =  0.016153454780578613\n",
      "51 % Trained loss =  0.014504652470350266\n",
      "52 % Trained loss =  0.02384544350206852\n",
      "53 % Trained loss =  0.019945327192544937\n",
      "54 % Trained loss =  0.011757371947169304\n",
      "55 % Trained loss =  0.011824307963252068\n",
      "56 % Trained loss =  0.020872311666607857\n",
      "57 % Trained loss =  0.01877087727189064\n",
      "58 % Trained loss =  0.010758702643215656\n",
      "59 % Trained loss =  0.01286613941192627\n",
      "60 % Trained loss =  0.012585436925292015\n",
      "61 % Trained loss =  0.012092461809515953\n",
      "62 % Trained loss =  0.013470829464495182\n",
      "63 % Trained loss =  0.009343326091766357\n",
      "64 % Trained loss =  0.010389196686446667\n",
      "65 % Trained loss =  0.0091813700273633\n",
      "66 % Trained loss =  0.007876659743487835\n",
      "67 % Trained loss =  0.009444604627788067\n",
      "68 % Trained loss =  0.0075645786710083485\n",
      "69 % Trained loss =  0.007402168121188879\n",
      "70 % Trained loss =  0.007068015169352293\n",
      "71 % Trained loss =  0.00683109275996685\n",
      "72 % Trained loss =  0.0060509624890983105\n",
      "73 % Trained loss =  0.006409512832760811\n",
      "74 % Trained loss =  0.005444378592073917\n",
      "75 % Trained loss =  0.004624922294169664\n",
      "76 % Trained loss =  0.004736320115625858\n",
      "77 % Trained loss =  0.005660269875079393\n",
      "78 % Trained loss =  0.0054754517041146755\n",
      "79 % Trained loss =  0.005144035909324884\n",
      "80 % Trained loss =  0.004228615202009678\n",
      "81 % Trained loss =  0.003220677375793457\n",
      "82 % Trained loss =  0.0047554573975503445\n",
      "83 % Trained loss =  0.003048848593607545\n",
      "84 % Trained loss =  0.004148303531110287\n",
      "85 % Trained loss =  0.004055259749293327\n",
      "86 % Trained loss =  0.00391728337854147\n",
      "87 % Trained loss =  0.003861198667436838\n",
      "88 % Trained loss =  0.0038609385956078768\n",
      "89 % Trained loss =  0.003073315368965268\n",
      "90 % Trained loss =  0.003207868430763483\n",
      "91 % Trained loss =  0.0033185055945068598\n",
      "92 % Trained loss =  0.003802292514592409\n",
      "93 % Trained loss =  0.004183997865766287\n",
      "94 % Trained loss =  0.0040985881350934505\n",
      "95 % Trained loss =  0.003101909765973687\n",
      "96 % Trained loss =  0.0031453052069991827\n",
      "97 % Trained loss =  0.0020342350471764803\n",
      "98 % Trained loss =  0.002880335785448551\n",
      "99 % Trained loss =  0.0024975163396447897\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "test_inputs = torch.from_numpy(x_test).view([-1, 1, dim]).float().cuda()\n",
    "test_labels = torch.from_numpy(y_test).view([-1, 1]).float().cuda()\n",
    "for batch in range(100):  # loop over the dataset multiple times\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    indices = np.random.choice(len(x_train), size=(30))\n",
    "    inputs = x_train[indices]\n",
    "    labels = y_train[indices]\n",
    "    \n",
    "    inputs = torch.from_numpy(inputs).float().cuda()\n",
    "    labels = torch.from_numpy(labels).float().cuda()\n",
    "    \n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(inputs).view([-1,1]).cuda()\n",
    "    loss = criterion(outputs, labels).cuda()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(batch, \"% Trained\", \"loss = \", loss.item())\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (l): Sequential(\n",
       "    (0): Linear(in_features=12634, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "  )\n",
       "  (r): Sequential(\n",
       "    (0): Res1d(\n",
       "      (l): Sequential(\n",
       "        (0): Conv1d(1, 4, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): InstanceNorm1d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (r1): Identity()\n",
       "      (r): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): Conv1d(1, 4, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): InstanceNorm1d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (1): Res1d(\n",
       "      (l): Sequential(\n",
       "        (0): Conv1d(4, 8, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): InstanceNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (r1): Identity()\n",
       "      (r): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): Conv1d(4, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): InstanceNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (2): Res1d(\n",
       "      (l): Sequential(\n",
       "        (0): Conv1d(8, 8, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "        (1): InstanceNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (r1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "      (r): Sequential(\n",
       "        (0): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "        (1): Conv1d(8, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): InstanceNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (3): Res1d(\n",
       "      (l): Sequential(\n",
       "        (0): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): InstanceNorm1d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (r1): Identity()\n",
       "      (r): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): Conv1d(8, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): InstanceNorm1d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (4): Res1d(\n",
       "      (l): Sequential(\n",
       "        (0): Conv1d(16, 16, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (1): InstanceNorm1d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (r1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "      (r): Sequential(\n",
       "        (0): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "        (1): Conv1d(16, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): InstanceNorm1d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (5): Res1d(\n",
       "      (l): Sequential(\n",
       "        (0): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (r1): Identity()\n",
       "      (r): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): Conv1d(16, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (6): Res1d(\n",
       "      (l): Sequential(\n",
       "        (0): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "        (1): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (r1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "      (r): Sequential(\n",
       "        (0): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "        (1): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (7): Res1d(\n",
       "      (l): Sequential(\n",
       "        (0): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (r1): Identity()\n",
       "      (r): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (8): Res1d(\n",
       "      (l): Sequential(\n",
       "        (0): Conv1d(64, 64, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (1): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (r1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "      (r): Sequential(\n",
       "        (0): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "        (1): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (9): Res1d(\n",
       "      (l): Sequential(\n",
       "        (0): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (r1): Identity()\n",
       "      (r): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (10): Res1d(\n",
       "      (l): Sequential(\n",
       "        (0): Conv1d(128, 128, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (1): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (r1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "      (r): Sequential(\n",
       "        (0): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "        (1): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (11): Res1d(\n",
       "      (l): Sequential(\n",
       "        (0): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (r1): Identity()\n",
       "      (r): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (12): Res1d(\n",
       "      (l): Sequential(\n",
       "        (0): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "        (1): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (r1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "      (r): Sequential(\n",
       "        (0): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "        (1): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (13): Res1d(\n",
       "      (l): Sequential(\n",
       "        (0): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (r1): Identity()\n",
       "      (r): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (14): Res1d(\n",
       "      (l): Sequential(\n",
       "        (0): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "        (1): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (r1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "      (r): Sequential(\n",
       "        (0): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "        (1): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (15): Res1d(\n",
       "      (l): Sequential(\n",
       "        (0): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "        (1): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (r1): Identity()\n",
       "      (r): Sequential(\n",
       "        (0): Identity()\n",
       "        (1): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (16): Res1d(\n",
       "      (l): Sequential(\n",
       "        (0): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "        (1): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (r1): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "      (r): Sequential(\n",
       "        (0): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "        (1): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "        (2): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "      )\n",
       "      (relu): ReLU()\n",
       "    )\n",
       "    (17): Flatten()\n",
       "  )\n",
       "  (last): Sequential(\n",
       "    (0): Linear(in_features=50240, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "    (3): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0041]],\n",
       "\n",
       "        [[0.0057]],\n",
       "\n",
       "        [[0.9967]],\n",
       "\n",
       "        [[0.9970]],\n",
       "\n",
       "        [[0.9970]],\n",
       "\n",
       "        [[0.0034]],\n",
       "\n",
       "        [[0.9967]],\n",
       "\n",
       "        [[0.9995]],\n",
       "\n",
       "        [[0.9969]],\n",
       "\n",
       "        [[0.0047]],\n",
       "\n",
       "        [[0.9980]],\n",
       "\n",
       "        [[0.9965]],\n",
       "\n",
       "        [[0.9975]],\n",
       "\n",
       "        [[0.0035]],\n",
       "\n",
       "        [[0.0029]],\n",
       "\n",
       "        [[0.9978]],\n",
       "\n",
       "        [[0.9969]],\n",
       "\n",
       "        [[0.9961]],\n",
       "\n",
       "        [[0.0083]],\n",
       "\n",
       "        [[0.9970]],\n",
       "\n",
       "        [[0.9966]],\n",
       "\n",
       "        [[0.9983]],\n",
       "\n",
       "        [[0.9961]],\n",
       "\n",
       "        [[0.9991]],\n",
       "\n",
       "        [[0.9986]],\n",
       "\n",
       "        [[0.9973]],\n",
       "\n",
       "        [[0.0016]],\n",
       "\n",
       "        [[0.9979]],\n",
       "\n",
       "        [[0.9971]],\n",
       "\n",
       "        [[0.0041]],\n",
       "\n",
       "        [[0.0053]],\n",
       "\n",
       "        [[0.9978]],\n",
       "\n",
       "        [[0.9987]],\n",
       "\n",
       "        [[0.0107]],\n",
       "\n",
       "        [[0.0046]],\n",
       "\n",
       "        [[0.0041]],\n",
       "\n",
       "        [[0.9974]],\n",
       "\n",
       "        [[0.9979]],\n",
       "\n",
       "        [[0.9960]],\n",
       "\n",
       "        [[0.9982]],\n",
       "\n",
       "        [[0.9965]],\n",
       "\n",
       "        [[0.0100]],\n",
       "\n",
       "        [[0.9897]],\n",
       "\n",
       "        [[0.9972]],\n",
       "\n",
       "        [[0.9982]],\n",
       "\n",
       "        [[0.0061]],\n",
       "\n",
       "        [[0.9977]],\n",
       "\n",
       "        [[0.0097]],\n",
       "\n",
       "        [[0.0074]],\n",
       "\n",
       "        [[0.9992]],\n",
       "\n",
       "        [[0.0079]],\n",
       "\n",
       "        [[0.0023]],\n",
       "\n",
       "        [[0.9949]],\n",
       "\n",
       "        [[0.9968]],\n",
       "\n",
       "        [[0.9977]],\n",
       "\n",
       "        [[0.0031]],\n",
       "\n",
       "        [[0.9954]],\n",
       "\n",
       "        [[0.9995]],\n",
       "\n",
       "        [[0.9980]],\n",
       "\n",
       "        [[0.0066]],\n",
       "\n",
       "        [[0.9987]],\n",
       "\n",
       "        [[0.9970]],\n",
       "\n",
       "        [[0.9980]],\n",
       "\n",
       "        [[0.9966]],\n",
       "\n",
       "        [[0.9985]],\n",
       "\n",
       "        [[0.0044]],\n",
       "\n",
       "        [[0.9972]],\n",
       "\n",
       "        [[0.0024]],\n",
       "\n",
       "        [[0.9971]],\n",
       "\n",
       "        [[0.0051]],\n",
       "\n",
       "        [[0.9967]],\n",
       "\n",
       "        [[0.0053]],\n",
       "\n",
       "        [[0.0059]],\n",
       "\n",
       "        [[0.0038]],\n",
       "\n",
       "        [[0.9963]],\n",
       "\n",
       "        [[0.9978]],\n",
       "\n",
       "        [[0.0045]],\n",
       "\n",
       "        [[0.0036]],\n",
       "\n",
       "        [[0.9981]],\n",
       "\n",
       "        [[0.0049]],\n",
       "\n",
       "        [[0.0041]],\n",
       "\n",
       "        [[0.0068]],\n",
       "\n",
       "        [[0.9972]],\n",
       "\n",
       "        [[0.9983]],\n",
       "\n",
       "        [[0.0094]],\n",
       "\n",
       "        [[0.9975]],\n",
       "\n",
       "        [[0.9992]],\n",
       "\n",
       "        [[0.9908]],\n",
       "\n",
       "        [[0.0127]],\n",
       "\n",
       "        [[0.0041]],\n",
       "\n",
       "        [[0.0028]],\n",
       "\n",
       "        [[0.9947]],\n",
       "\n",
       "        [[0.0037]],\n",
       "\n",
       "        [[0.9983]],\n",
       "\n",
       "        [[0.9989]],\n",
       "\n",
       "        [[0.0026]],\n",
       "\n",
       "        [[0.9914]],\n",
       "\n",
       "        [[0.0044]],\n",
       "\n",
       "        [[0.0085]],\n",
       "\n",
       "        [[0.9978]],\n",
       "\n",
       "        [[0.0041]],\n",
       "\n",
       "        [[0.9968]],\n",
       "\n",
       "        [[0.0064]],\n",
       "\n",
       "        [[0.0023]],\n",
       "\n",
       "        [[0.9985]],\n",
       "\n",
       "        [[0.9932]],\n",
       "\n",
       "        [[0.9985]],\n",
       "\n",
       "        [[0.9971]],\n",
       "\n",
       "        [[0.9955]],\n",
       "\n",
       "        [[0.9980]],\n",
       "\n",
       "        [[0.0019]],\n",
       "\n",
       "        [[0.9989]],\n",
       "\n",
       "        [[0.9992]],\n",
       "\n",
       "        [[0.9964]],\n",
       "\n",
       "        [[0.0078]],\n",
       "\n",
       "        [[0.9980]],\n",
       "\n",
       "        [[0.0063]],\n",
       "\n",
       "        [[0.9983]],\n",
       "\n",
       "        [[0.0042]],\n",
       "\n",
       "        [[0.9969]],\n",
       "\n",
       "        [[0.0024]],\n",
       "\n",
       "        [[0.9982]],\n",
       "\n",
       "        [[0.9991]],\n",
       "\n",
       "        [[0.9972]],\n",
       "\n",
       "        [[0.0264]],\n",
       "\n",
       "        [[0.9987]],\n",
       "\n",
       "        [[0.9984]],\n",
       "\n",
       "        [[0.9970]],\n",
       "\n",
       "        [[0.0048]],\n",
       "\n",
       "        [[0.0034]],\n",
       "\n",
       "        [[0.9967]],\n",
       "\n",
       "        [[0.0042]],\n",
       "\n",
       "        [[0.9929]],\n",
       "\n",
       "        [[0.9982]],\n",
       "\n",
       "        [[0.9980]],\n",
       "\n",
       "        [[0.9964]],\n",
       "\n",
       "        [[0.0060]],\n",
       "\n",
       "        [[0.0056]],\n",
       "\n",
       "        [[0.9968]],\n",
       "\n",
       "        [[0.9923]],\n",
       "\n",
       "        [[0.9977]],\n",
       "\n",
       "        [[0.0056]],\n",
       "\n",
       "        [[0.0044]],\n",
       "\n",
       "        [[0.9977]],\n",
       "\n",
       "        [[0.9987]],\n",
       "\n",
       "        [[0.9974]],\n",
       "\n",
       "        [[0.9945]],\n",
       "\n",
       "        [[0.9953]],\n",
       "\n",
       "        [[0.9974]],\n",
       "\n",
       "        [[0.0108]],\n",
       "\n",
       "        [[0.0072]],\n",
       "\n",
       "        [[0.9969]],\n",
       "\n",
       "        [[0.9969]],\n",
       "\n",
       "        [[0.9985]],\n",
       "\n",
       "        [[0.9981]],\n",
       "\n",
       "        [[0.9982]],\n",
       "\n",
       "        [[0.9963]],\n",
       "\n",
       "        [[0.9994]],\n",
       "\n",
       "        [[0.0038]],\n",
       "\n",
       "        [[0.9940]],\n",
       "\n",
       "        [[0.9968]],\n",
       "\n",
       "        [[0.0248]],\n",
       "\n",
       "        [[0.9970]],\n",
       "\n",
       "        [[0.9923]],\n",
       "\n",
       "        [[0.9983]],\n",
       "\n",
       "        [[0.0065]],\n",
       "\n",
       "        [[0.9977]],\n",
       "\n",
       "        [[0.9972]],\n",
       "\n",
       "        [[0.0049]],\n",
       "\n",
       "        [[0.9983]],\n",
       "\n",
       "        [[0.9979]],\n",
       "\n",
       "        [[0.9957]],\n",
       "\n",
       "        [[0.0105]],\n",
       "\n",
       "        [[0.9963]],\n",
       "\n",
       "        [[0.9949]],\n",
       "\n",
       "        [[0.0061]],\n",
       "\n",
       "        [[0.0047]],\n",
       "\n",
       "        [[0.9957]],\n",
       "\n",
       "        [[0.0102]],\n",
       "\n",
       "        [[0.9970]],\n",
       "\n",
       "        [[0.9981]],\n",
       "\n",
       "        [[0.3927]],\n",
       "\n",
       "        [[0.8812]],\n",
       "\n",
       "        [[0.6703]],\n",
       "\n",
       "        [[0.9689]],\n",
       "\n",
       "        [[0.8285]],\n",
       "\n",
       "        [[0.7597]],\n",
       "\n",
       "        [[0.7357]],\n",
       "\n",
       "        [[0.7800]],\n",
       "\n",
       "        [[0.6864]],\n",
       "\n",
       "        [[0.8599]],\n",
       "\n",
       "        [[0.9525]],\n",
       "\n",
       "        [[0.8916]],\n",
       "\n",
       "        [[0.9734]],\n",
       "\n",
       "        [[0.8590]],\n",
       "\n",
       "        [[0.9520]],\n",
       "\n",
       "        [[0.9455]],\n",
       "\n",
       "        [[0.6068]],\n",
       "\n",
       "        [[0.9324]],\n",
       "\n",
       "        [[0.7814]],\n",
       "\n",
       "        [[0.9548]],\n",
       "\n",
       "        [[0.9527]],\n",
       "\n",
       "        [[0.9768]],\n",
       "\n",
       "        [[0.8809]],\n",
       "\n",
       "        [[0.7739]],\n",
       "\n",
       "        [[0.9301]],\n",
       "\n",
       "        [[0.8352]],\n",
       "\n",
       "        [[0.7907]],\n",
       "\n",
       "        [[0.7037]],\n",
       "\n",
       "        [[0.8564]],\n",
       "\n",
       "        [[0.8016]],\n",
       "\n",
       "        [[0.8014]],\n",
       "\n",
       "        [[0.9017]],\n",
       "\n",
       "        [[0.9192]],\n",
       "\n",
       "        [[0.8032]],\n",
       "\n",
       "        [[0.8319]],\n",
       "\n",
       "        [[0.8994]],\n",
       "\n",
       "        [[0.9705]],\n",
       "\n",
       "        [[0.5984]],\n",
       "\n",
       "        [[0.9239]],\n",
       "\n",
       "        [[0.8573]],\n",
       "\n",
       "        [[0.9528]],\n",
       "\n",
       "        [[0.9716]],\n",
       "\n",
       "        [[0.9197]],\n",
       "\n",
       "        [[0.6132]],\n",
       "\n",
       "        [[0.7350]],\n",
       "\n",
       "        [[0.9764]]], device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = data_torch.cuda()\n",
    "net(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of the secure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "def connect_to_workers(n_workers):\n",
    "    return [\n",
    "        sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
    "        for i in range(n_workers)\n",
    "    ]\n",
    "\n",
    "def connect_to_crypto_provider():\n",
    "    return sy.VirtualWorker(hook, id=\"crypto_provider\")\n",
    "\n",
    "workers = connect_to_workers(n_workers=2)\n",
    "crypto_provider = connect_to_crypto_provider()\n",
    "\n",
    "def get_private_data_loaders(precision_fractional, workers, crypto_provider):\n",
    "    \n",
    "    def secret_share(tensor): #Transforms to fixed precision and secret share a tensor\n",
    "        return (\n",
    "            tensor\n",
    "            .fix_precision(precision_fractional=precision_fractional)\n",
    "            .share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "        )\n",
    "    \n",
    "    private_train_loader = [\n",
    "        (secret_share(torch.Tensor(x_train[i*5:i*5+5])), secret_share(torch.Tensor(y_train[i*5:i*5+5])))\n",
    "        for i in range (n_train_items)\n",
    "        if i < n_train_items / 5\n",
    "    ]\n",
    "    \n",
    "    private_test_loader = [\n",
    "        (secret_share(torch.Tensor(x_test[i*5:i*5+5])), secret_share(torch.Tensor(y_test[i*5:i*5+5])))\n",
    "        for i in range (n_train_items)\n",
    "        if i < n_train_items / 5\n",
    "    ]\n",
    "    return private_train_loader, private_test_loader\n",
    "    \n",
    "    \n",
    "private_train_loader, private_test_loader = get_private_data_loaders(\n",
    "    precision_fractional=3,\n",
    "    workers=workers,\n",
    "    crypto_provider=crypto_provider\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for encrypted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res1d(nn.Module):\n",
    "    # the conv layers\n",
    "    def __init__(self, inSize, outSize, kernel=(3,), strides=1,):\n",
    "        super(Res1d, self).__init__()\n",
    "        self.inSize = inSize\n",
    "        self.outSize = outSize\n",
    "        # hard-coded to do the padding correctly\n",
    "        if inSize in (16,64,128,512) and strides is 2:\n",
    "            pding = 0\n",
    "        else:\n",
    "            pding = 1\n",
    "        self.l1 = nn.Conv1d(inSize, outSize, kernel, stride=strides, padding=pding, bias=False)\n",
    "        self.l2 = nn.Identity()\n",
    "        \n",
    "        if strides > 1 or inSize != outSize:\n",
    "            if strides > 1:\n",
    "                self.r1 = nn.Identity()\n",
    "                self.r2 = nn.AvgPool1d(strides)\n",
    "            else:\n",
    "                self.r1 = None\n",
    "                self.r2 = None\n",
    "            self.r3 = nn.Conv1d(inSize, outSize, 1, bias=False)\n",
    "            self.r4 = nn.Identity()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        l = x\n",
    "        l = self.l1(l)\n",
    "        l = self.l2(l)\n",
    "        \n",
    "        if self.r1 is not None:\n",
    "            r = self.r1(x)\n",
    "            r = self.r2(r)\n",
    "            r = self.r3(r)\n",
    "            r = self.r4(r)\n",
    "        else:\n",
    "            r = self.r3(x)\n",
    "            r = self.r4(r)\n",
    "            \n",
    "        x = l + r\n",
    "        print(\"forwarding: \", self.inSize, self.outSize)\n",
    "        return F.relu(x)\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(dim, 64)\n",
    "        self.l2 = F.relu\n",
    "        \n",
    "        self.r1 = Res1d(1, 4, 3)\n",
    "        \n",
    "        self.r2 = Res1d(4, 8, 3)\n",
    "        self.r3 = Res1d(8, 8, 3, strides=2)\n",
    "        \n",
    "        self.r4 = Res1d(8, 16, 3)\n",
    "        self.r5 = Res1d(16, 16, 3, strides=2)\n",
    "        \n",
    "        self.r6 = Res1d(16, 32, 3)\n",
    "        self.r7 = Res1d(32, 32, 3, strides=2)\n",
    "        \n",
    "        self.r8 = Res1d(32, 64, 3)\n",
    "        self.r9 = Res1d(64, 64, 3, strides=2)\n",
    "        \n",
    "        self.r10 = Res1d(64, 128, 3)\n",
    "        self.r11 = Res1d(128, 128, 3, strides=2)\n",
    "        \n",
    "        self.r12 = Res1d(128, 256, 3)\n",
    "        self.r13 = Res1d(256, 256, 3, strides=2)\n",
    "        \n",
    "        self.r14 = Res1d(256, 512, 3)\n",
    "        self.r15 = Res1d(512, 512, 3, strides=2)\n",
    "        \n",
    "        self.r16 = Res1d(512, 1024, 3)\n",
    "        self.r17 = Res1d(1024, 1024, 3, strides=2)\n",
    "        \n",
    "        # size is by experiment and hardcode\n",
    "        self.lastLinear = nn.Linear(50240,32)\n",
    "        self.lastRelu = F.relu\n",
    "        self.lastAgg = nn.Linear(32,1)\n",
    "        self.lastSigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):           \n",
    "        # shape is (batch, channel, time)\n",
    "        l = x\n",
    "        l = x.view(x.shape[0],-1)\n",
    "        l = self.l1(l)\n",
    "        l = self.l2(l)\n",
    "\n",
    "        # conv layers should operate on time\n",
    "        r = x\n",
    "        r = self.r1(r)\n",
    "        r = self.r4(self.r3(self.r2(r)))\n",
    "        r = self.r8(self.r7(self.r6(self.r5(r))))\n",
    "        r = self.r12(self.r11(self.r10(self.r9(r))))\n",
    "        r = self.r16(self.r15(self.r14(self.r13(r))))\n",
    "        r = self.r17(r)\n",
    "        \n",
    "        # flatten l\n",
    "        r = r.view(x.shape[0],-1)\n",
    "        l = l.view(x.shape[0],-1)\n",
    "        y = torch.cat((l,r),dim=1)\n",
    "        y = self.lastLinear(y)\n",
    "        y = self.lastRelu(y)\n",
    "        y = self.lastAgg(y)\n",
    "        y = self.lastSigmoid(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Private Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, private_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(private_train_loader): # <-- now it is a private dataset\n",
    "        print('training...')\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        batch_size = output.shape[0]\n",
    "        loss = ((output - target)**2).sum().refresh()/batch_size\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "#Although the aim of this project was to provide private training, we prepared code for testing as well\n",
    "def test(model, private_test_loader):\n",
    "    print('testing...')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in private_test_loader:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "    correct = correct.get().float_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "testing...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n"
     ]
    },
        {
     "ename": "RuntimeError",
     "evalue": "shape '[1]' is invalid for input of size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b36cc8ffdaa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate_test_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-118cd7fa07be>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, private_test_loader)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0;31m# Send the new command to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_syft_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;31m# Send it to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# Put back SyftTensor on the tensors found in the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_syft_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;31m# Send it to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# Put back SyftTensor on the tensors found in the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_attr\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_self\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0;31m# Put back AdditiveSharingTensor on the tensors found in the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_pointer_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, recipient, message, return_ids)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mResponseSignatureError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mbin_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# Step 3: deserialize the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_send_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mVirtualWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWorker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFederatedClient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseWorker\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_recv_msg\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mrecv_msg\u001b[0;34m(self, bin_message)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"worker {self} received {sy.codes.code2MSGTYPE[msg_type]} {contents}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;31m# Step 1: route message to appropriate function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_router\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmsg_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Step 2: Serialize the message to simple python objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mexecute_command\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                     \u001b[0;31m# TODO Andrew thinks this is gross, please fix. Instead need to properly deserialize strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                     \u001b[0;31m# we can make some errors more descriptive with this method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mroute_method_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# means that there is a wrapper to remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                     \u001b[0;31m# we can make some errors more descriptive with this method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1]' is invalid for input of size 5"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model = model.fix_precision().share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "optimizer = optimizer.fix_precision() \n",
    "for epoch in range(1, 2):\n",
    "    train(model, private_train_loader, optimizer, epoch)\n",
    "    test(model, private_test_loader)"
   ]
  },
 
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
