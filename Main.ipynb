{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and training Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/h/houruomu/.local/lib/python3.6/site-packages/tf_encrypted/session.py:24: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "torch version: 1.3.0\n",
      "syft version: 0.2.0a2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import syft as sy\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import time\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"syft version:\", sy.__version__)\n",
    "\n",
    "#All warnings resulting from running these imports are normal due to tensorflow moving from 1.x to 2.0 as well as numpy deprecating a few commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSamples(filename):\n",
    "    data = pd.read_csv(filename, sep='\\t')\n",
    "    return data.values[:,1:].transpose()\n",
    "\n",
    "dim = 12634\n",
    "data1 = getSamples(\"GSE2034-Normal-train.txt\")\n",
    "data2 = getSamples(\"GSE2034-Tumor-train.txt\")\n",
    "\n",
    "data1Label = np.zeros(len(data1)).reshape((-1, 1))\n",
    "data2Label = np.ones(len(data2)).reshape((-1, 1))\n",
    "x = np.concatenate((data1, data2))\n",
    "y = np.concatenate((data1Label, data2Label))\n",
    "\n",
    "# shuffle the data\n",
    "idx = np.random.permutation(len(x))\n",
    "x,y = x[idx], y[idx]\n",
    "\n",
    "z = np.concatenate((x, y), axis = 1)\n",
    "\n",
    "# We follow an 80/20 partitioning for the training and testing sets\n",
    "n_train_items = 181\n",
    "n_test_items = 46\n",
    "\n",
    "# partition the data into training data and test data\n",
    "x_train = x[:n_train_items]\n",
    "y_train = y[:n_train_items]\n",
    "\n",
    "x_train = x_train.reshape((-1,1,dim))\n",
    "y_train = y_train.reshape((-1,1))\n",
    "\n",
    "x_test = x[n_train_items:]\n",
    "y_test = y[n_train_items:]    \n",
    "\n",
    "x_test = x_test.reshape((-1,1,dim))\n",
    "y_test = y_test.reshape((-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data into (batch, channel = 1, length=dim)\n",
    "data_torch = torch.from_numpy(x).view([-1, 1, dim]).float()\n",
    "label_torch = torch.from_numpy(y).view([-1,1,1]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Model applied to GSE2034"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 12634\n",
    "\n",
    "class Res1d(nn.Module):\n",
    "    # the conv layers\n",
    "    def __init__(self, inSize, outSize, kernel=(3,), strides=1,):\n",
    "        super(Res1d, self).__init__()\n",
    "        # hard-coded to do the padding correctly\n",
    "        if inSize in (16,64,128,512) and strides is 2:\n",
    "            pding = 0\n",
    "        else:\n",
    "            pding = 1\n",
    "        self.l1 = nn.Conv1d(inSize, outSize, kernel, stride=strides, padding=pding, bias=False)\n",
    "        self.l2 = nn.InstanceNorm1d(outSize)\n",
    "        \n",
    "        if strides > 1 or inSize != outSize:\n",
    "            if strides > 1:\n",
    "                self.r1 = nn.Identity()\n",
    "                self.r2 = nn.AvgPool1d(strides)\n",
    "            else:\n",
    "                self.r1 = None\n",
    "                self.r2 = None\n",
    "            self.r3 = nn.Conv1d(inSize, outSize, 1, bias=False)\n",
    "            self.r4 = nn.InstanceNorm1d(outSize)\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l = self.l1(x)\n",
    "        l = self.l2(l)\n",
    "        \n",
    "        if self.r1 is not None:\n",
    "            r = self.r1(x)\n",
    "            r = self.r2(r)\n",
    "            r = self.r3(r)\n",
    "            r = self.r4(r)\n",
    "        else:\n",
    "            r = self.r3(x)\n",
    "            r = self.r4(r)\n",
    "            \n",
    "        x = l + r\n",
    "        return self.relu(x)\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(dim, 64)\n",
    "        self.l2 = nn.ReLU()\n",
    "        \n",
    "        self.r1 = Res1d(1, 4, 3)\n",
    "        \n",
    "        self.r2 = Res1d(4, 8, 3)\n",
    "        self.r3 = Res1d(8, 8, 3, strides=2)\n",
    "        \n",
    "        self.r4 = Res1d(8, 16, 3)\n",
    "        self.r5 = Res1d(16, 16, 3, strides=2)\n",
    "        \n",
    "        self.r6 = Res1d(16, 32, 3)\n",
    "        self.r7 = Res1d(32, 32, 3, strides=2)\n",
    "        \n",
    "        self.r8 = Res1d(32, 64, 3)\n",
    "        self.r9 = Res1d(64, 64, 3, strides=2)\n",
    "        \n",
    "        self.r10 = Res1d(64, 128, 3)\n",
    "        self.r11 = Res1d(128, 128, 3, strides=2)\n",
    "        \n",
    "        self.r12 = Res1d(128, 256, 3)\n",
    "        self.r13 = Res1d(256, 256, 3, strides=2)\n",
    "        \n",
    "        self.r14 = Res1d(256, 512, 3)\n",
    "        self.r15 = Res1d(512, 512, 3, strides=2)\n",
    "        \n",
    "        self.r16 = Res1d(512, 1024, 3)\n",
    "        self.r17 = Res1d(1024, 1024, 3, strides=2)\n",
    "        \n",
    "        # size is by experiment and hardcode\n",
    "        self.lastLinear = nn.Linear(50240,32)\n",
    "        self.lastRelu = nn.ReLU()\n",
    "        self.lastAgg = nn.Linear(32,1)\n",
    "        self.lastSigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # shape is (batch, channel, time)\n",
    "        l = x\n",
    "        l = self.l2(self.l1(l))\n",
    "        \n",
    "        # conv layers should operate on time\n",
    "        r = x\n",
    "        r = self.r4(self.r3(self.r2(self.r1(r))))\n",
    "        r = self.r8(self.r7(self.r6(self.r5(r))))\n",
    "        r = self.r12(self.r11(self.r10(self.r9(r))))\n",
    "        r = self.r16(self.r15(self.r14(self.r13(r))))\n",
    "        r = self.r17(r)\n",
    "        \n",
    "        # flatten l\n",
    "        r = r.view(x.shape[0],1, -1)\n",
    "        y = torch.cat((l,r),-1)\n",
    "        y = self.lastLinear(y)\n",
    "        y = self.lastRelu(y)\n",
    "        y = self.lastAgg(y)\n",
    "        y = self.lastSigmoid(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy \n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on plain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 % Trained loss =  0.752633810043335\n",
      "1 % Trained loss =  0.6460354328155518\n",
      "2 % Trained loss =  0.515184760093689\n",
      "3 % Trained loss =  0.7169057726860046\n",
      "4 % Trained loss =  0.6461413502693176\n",
      "5 % Trained loss =  0.6098722815513611\n",
      "6 % Trained loss =  0.6529352068901062\n",
      "7 % Trained loss =  0.6020980477333069\n",
      "8 % Trained loss =  0.6127597689628601\n",
      "9 % Trained loss =  0.5221014618873596\n",
      "10 % Trained loss =  0.5603885054588318\n",
      "11 % Trained loss =  0.5306809544563293\n",
      "12 % Trained loss =  0.6199235320091248\n",
      "13 % Trained loss =  0.4783308804035187\n",
      "14 % Trained loss =  0.34084373712539673\n",
      "15 % Trained loss =  0.5322335958480835\n",
      "16 % Trained loss =  0.619378387928009\n",
      "17 % Trained loss =  0.4341699182987213\n",
      "18 % Trained loss =  0.4259912371635437\n",
      "19 % Trained loss =  0.43842047452926636\n",
      "20 % Trained loss =  0.40366554260253906\n",
      "21 % Trained loss =  0.43456241488456726\n",
      "22 % Trained loss =  0.3753505051136017\n",
      "23 % Trained loss =  0.37445008754730225\n",
      "24 % Trained loss =  0.3321376144886017\n",
      "25 % Trained loss =  0.3021309971809387\n",
      "26 % Trained loss =  0.3071903884410858\n",
      "27 % Trained loss =  0.34207937121391296\n",
      "28 % Trained loss =  0.25197580456733704\n",
      "29 % Trained loss =  0.27598387002944946\n",
      "30 % Trained loss =  0.21870940923690796\n",
      "31 % Trained loss =  0.1970418244600296\n",
      "32 % Trained loss =  0.19187478721141815\n",
      "33 % Trained loss =  0.17618663609027863\n",
      "34 % Trained loss =  0.17254473268985748\n",
      "35 % Trained loss =  0.12132442742586136\n",
      "36 % Trained loss =  0.12379887700080872\n",
      "37 % Trained loss =  0.10864714533090591\n",
      "38 % Trained loss =  0.11422815173864365\n",
      "39 % Trained loss =  0.07580806314945221\n",
      "40 % Trained loss =  0.09399150311946869\n",
      "41 % Trained loss =  0.07044761627912521\n",
      "42 % Trained loss =  0.05629748851060867\n",
      "43 % Trained loss =  0.0531342513859272\n",
      "44 % Trained loss =  0.0566164031624794\n",
      "45 % Trained loss =  0.04638083651661873\n",
      "46 % Trained loss =  0.05353949964046478\n",
      "47 % Trained loss =  0.06431367248296738\n",
      "48 % Trained loss =  0.05198882892727852\n",
      "49 % Trained loss =  0.035687193274497986\n",
      "50 % Trained loss =  0.058262307196855545\n",
      "51 % Trained loss =  0.03142397850751877\n",
      "52 % Trained loss =  0.02618153765797615\n",
      "53 % Trained loss =  0.026582811027765274\n",
      "54 % Trained loss =  0.020185144618153572\n",
      "55 % Trained loss =  0.020565466955304146\n",
      "56 % Trained loss =  0.022803111001849174\n",
      "57 % Trained loss =  0.02422475628554821\n",
      "58 % Trained loss =  0.016789635643363\n",
      "59 % Trained loss =  0.014539738185703754\n",
      "60 % Trained loss =  0.013400882482528687\n",
      "61 % Trained loss =  0.017102662473917007\n",
      "62 % Trained loss =  0.017438502982258797\n",
      "63 % Trained loss =  0.01213681697845459\n",
      "64 % Trained loss =  0.011241803877055645\n",
      "65 % Trained loss =  0.008742601610720158\n",
      "66 % Trained loss =  0.008997241966426373\n",
      "67 % Trained loss =  0.007582277525216341\n",
      "68 % Trained loss =  0.007900060154497623\n",
      "69 % Trained loss =  0.01447310671210289\n",
      "70 % Trained loss =  0.014411156997084618\n",
      "71 % Trained loss =  0.009398013353347778\n",
      "72 % Trained loss =  0.008417815901339054\n",
      "73 % Trained loss =  0.009395700879395008\n",
      "74 % Trained loss =  0.007759575266391039\n",
      "75 % Trained loss =  0.006908501964062452\n",
      "76 % Trained loss =  0.008828476071357727\n",
      "77 % Trained loss =  0.007205241825431585\n",
      "78 % Trained loss =  0.007391699124127626\n",
      "79 % Trained loss =  0.007682064548134804\n",
      "80 % Trained loss =  0.00599424634128809\n",
      "81 % Trained loss =  0.005733048543334007\n",
      "82 % Trained loss =  0.004322818014770746\n",
      "83 % Trained loss =  0.005248456262052059\n",
      "84 % Trained loss =  0.004718033596873283\n",
      "85 % Trained loss =  0.008681039325892925\n",
      "86 % Trained loss =  0.0039953868836164474\n",
      "87 % Trained loss =  0.004246068187057972\n",
      "88 % Trained loss =  0.005031449254602194\n",
      "89 % Trained loss =  0.004976136609911919\n",
      "90 % Trained loss =  0.005458414554595947\n",
      "91 % Trained loss =  0.0049623954109847546\n",
      "92 % Trained loss =  0.005351812578737736\n",
      "93 % Trained loss =  0.004640393890440464\n",
      "94 % Trained loss =  0.004331581294536591\n",
      "95 % Trained loss =  0.003621283220127225\n",
      "96 % Trained loss =  0.004294974729418755\n",
      "97 % Trained loss =  0.0038516572676599026\n",
      "98 % Trained loss =  0.004449180793017149\n",
      "99 % Trained loss =  0.0037690033204853535\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "test_inputs = torch.from_numpy(x_test).view([-1, 1, dim]).float().cuda()\n",
    "test_labels = torch.from_numpy(y_test).view([-1, 1]).float().cuda()\n",
    "for batch in range(100):  # loop over the dataset multiple times\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    indices = np.random.choice(len(x_train), size=(30))\n",
    "    inputs = x_train[indices]\n",
    "    labels = y_train[indices]\n",
    "    \n",
    "    inputs = torch.from_numpy(inputs).float().cuda()\n",
    "    labels = torch.from_numpy(labels).float().cuda()\n",
    "    \n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(inputs).view([-1,1]).cuda()\n",
    "    loss = criterion(outputs, labels).cuda()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(batch, \"% Trained\", \"loss = \", loss.item())\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (l1): Linear(in_features=12634, out_features=64, bias=True)\n",
       "  (l2): ReLU()\n",
       "  (r1): Res1d(\n",
       "    (l1): Conv1d(1, 4, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(1, 4, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r2): Res1d(\n",
       "    (l1): Conv1d(4, 8, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(4, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r3): Res1d(\n",
       "    (l1): Conv1d(8, 8, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r1): Identity()\n",
       "    (r2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (r3): Conv1d(8, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r4): Res1d(\n",
       "    (l1): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(8, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r5): Res1d(\n",
       "    (l1): Conv1d(16, 16, kernel_size=(3,), stride=(2,), bias=False)\n",
       "    (l2): InstanceNorm1d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r1): Identity()\n",
       "    (r2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (r3): Conv1d(16, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r6): Res1d(\n",
       "    (l1): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(16, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r7): Res1d(\n",
       "    (l1): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r1): Identity()\n",
       "    (r2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (r3): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r8): Res1d(\n",
       "    (l1): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r9): Res1d(\n",
       "    (l1): Conv1d(64, 64, kernel_size=(3,), stride=(2,), bias=False)\n",
       "    (l2): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r1): Identity()\n",
       "    (r2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (r3): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r10): Res1d(\n",
       "    (l1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r11): Res1d(\n",
       "    (l1): Conv1d(128, 128, kernel_size=(3,), stride=(2,), bias=False)\n",
       "    (l2): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r1): Identity()\n",
       "    (r2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (r3): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r12): Res1d(\n",
       "    (l1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r13): Res1d(\n",
       "    (l1): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r1): Identity()\n",
       "    (r2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (r3): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r14): Res1d(\n",
       "    (l1): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r15): Res1d(\n",
       "    (l1): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "    (l2): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r1): Identity()\n",
       "    (r2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (r3): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r16): Res1d(\n",
       "    (l1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r17): Res1d(\n",
       "    (l1): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r1): Identity()\n",
       "    (r2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (r3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (lastLinear): Linear(in_features=50240, out_features=32, bias=True)\n",
       "  (lastRelu): ReLU()\n",
       "  (lastAgg): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (lastSigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0041]],\n",
       "\n",
       "        [[0.0057]],\n",
       "\n",
       "        [[0.9967]],\n",
       "\n",
       "        [[0.9970]],\n",
       "\n",
       "        [[0.9970]],\n",
       "\n",
       "        [[0.0034]],\n",
       "\n",
       "        [[0.9967]],\n",
       "\n",
       "        [[0.9995]],\n",
       "\n",
       "        [[0.9969]],\n",
       "\n",
       "        [[0.0047]],\n",
       "\n",
       "        [[0.9980]],\n",
       "\n",
       "        [[0.9965]],\n",
       "\n",
       "        [[0.9975]],\n",
       "\n",
       "        [[0.0035]],\n",
       "\n",
       "        [[0.0029]],\n",
       "\n",
       "        [[0.9978]],\n",
       "\n",
       "        [[0.9969]],\n",
       "\n",
       "        [[0.9961]],\n",
       "\n",
       "        [[0.0083]],\n",
       "\n",
       "        [[0.9970]],\n",
       "\n",
       "        [[0.9966]],\n",
       "\n",
       "        [[0.9983]],\n",
       "\n",
       "        [[0.9961]],\n",
       "\n",
       "        [[0.9991]],\n",
       "\n",
       "        [[0.9986]],\n",
       "\n",
       "        [[0.9973]],\n",
       "\n",
       "        [[0.0016]],\n",
       "\n",
       "        [[0.9979]],\n",
       "\n",
       "        [[0.9971]],\n",
       "\n",
       "        [[0.0041]],\n",
       "\n",
       "        [[0.0053]],\n",
       "\n",
       "        [[0.9978]],\n",
       "\n",
       "        [[0.9987]],\n",
       "\n",
       "        [[0.0107]],\n",
       "\n",
       "        [[0.0046]],\n",
       "\n",
       "        [[0.0041]],\n",
       "\n",
       "        [[0.9974]],\n",
       "\n",
       "        [[0.9979]],\n",
       "\n",
       "        [[0.9960]],\n",
       "\n",
       "        [[0.9982]],\n",
       "\n",
       "        [[0.9965]],\n",
       "\n",
       "        [[0.0100]],\n",
       "\n",
       "        [[0.9897]],\n",
       "\n",
       "        [[0.9972]],\n",
       "\n",
       "        [[0.9982]],\n",
       "\n",
       "        [[0.0061]],\n",
       "\n",
       "        [[0.9977]],\n",
       "\n",
       "        [[0.0097]],\n",
       "\n",
       "        [[0.0074]],\n",
       "\n",
       "        [[0.9992]],\n",
       "\n",
       "        [[0.0079]],\n",
       "\n",
       "        [[0.0023]],\n",
       "\n",
       "        [[0.9949]],\n",
       "\n",
       "        [[0.9968]],\n",
       "\n",
       "        [[0.9977]],\n",
       "\n",
       "        [[0.0031]],\n",
       "\n",
       "        [[0.9954]],\n",
       "\n",
       "        [[0.9995]],\n",
       "\n",
       "        [[0.9980]],\n",
       "\n",
       "        [[0.0066]],\n",
       "\n",
       "        [[0.9987]],\n",
       "\n",
       "        [[0.9970]],\n",
       "\n",
       "        [[0.9980]],\n",
       "\n",
       "        [[0.9966]],\n",
       "\n",
       "        [[0.9985]],\n",
       "\n",
       "        [[0.0044]],\n",
       "\n",
       "        [[0.9972]],\n",
       "\n",
       "        [[0.0024]],\n",
       "\n",
       "        [[0.9971]],\n",
       "\n",
       "        [[0.0051]],\n",
       "\n",
       "        [[0.9967]],\n",
       "\n",
       "        [[0.0053]],\n",
       "\n",
       "        [[0.0059]],\n",
       "\n",
       "        [[0.0038]],\n",
       "\n",
       "        [[0.9963]],\n",
       "\n",
       "        [[0.9978]],\n",
       "\n",
       "        [[0.0045]],\n",
       "\n",
       "        [[0.0036]],\n",
       "\n",
       "        [[0.9981]],\n",
       "\n",
       "        [[0.0049]],\n",
       "\n",
       "        [[0.0041]],\n",
       "\n",
       "        [[0.0068]],\n",
       "\n",
       "        [[0.9972]],\n",
       "\n",
       "        [[0.9983]],\n",
       "\n",
       "        [[0.0094]],\n",
       "\n",
       "        [[0.9975]],\n",
       "\n",
       "        [[0.9992]],\n",
       "\n",
       "        [[0.9908]],\n",
       "\n",
       "        [[0.0127]],\n",
       "\n",
       "        [[0.0041]],\n",
       "\n",
       "        [[0.0028]],\n",
       "\n",
       "        [[0.9947]],\n",
       "\n",
       "        [[0.0037]],\n",
       "\n",
       "        [[0.9983]],\n",
       "\n",
       "        [[0.9989]],\n",
       "\n",
       "        [[0.0026]],\n",
       "\n",
       "        [[0.9914]],\n",
       "\n",
       "        [[0.0044]],\n",
       "\n",
       "        [[0.0085]],\n",
       "\n",
       "        [[0.9978]],\n",
       "\n",
       "        [[0.0041]],\n",
       "\n",
       "        [[0.9968]],\n",
       "\n",
       "        [[0.0064]],\n",
       "\n",
       "        [[0.0023]],\n",
       "\n",
       "        [[0.9985]],\n",
       "\n",
       "        [[0.9932]],\n",
       "\n",
       "        [[0.9985]],\n",
       "\n",
       "        [[0.9971]],\n",
       "\n",
       "        [[0.9955]],\n",
       "\n",
       "        [[0.9980]],\n",
       "\n",
       "        [[0.0019]],\n",
       "\n",
       "        [[0.9989]],\n",
       "\n",
       "        [[0.9992]],\n",
       "\n",
       "        [[0.9964]],\n",
       "\n",
       "        [[0.0078]],\n",
       "\n",
       "        [[0.9980]],\n",
       "\n",
       "        [[0.0063]],\n",
       "\n",
       "        [[0.9983]],\n",
       "\n",
       "        [[0.0042]],\n",
       "\n",
       "        [[0.9969]],\n",
       "\n",
       "        [[0.0024]],\n",
       "\n",
       "        [[0.9982]],\n",
       "\n",
       "        [[0.9991]],\n",
       "\n",
       "        [[0.9972]],\n",
       "\n",
       "        [[0.0264]],\n",
       "\n",
       "        [[0.9987]],\n",
       "\n",
       "        [[0.9984]],\n",
       "\n",
       "        [[0.9970]],\n",
       "\n",
       "        [[0.0048]],\n",
       "\n",
       "        [[0.0034]],\n",
       "\n",
       "        [[0.9967]],\n",
       "\n",
       "        [[0.0042]],\n",
       "\n",
       "        [[0.9929]],\n",
       "\n",
       "        [[0.9982]],\n",
       "\n",
       "        [[0.9980]],\n",
       "\n",
       "        [[0.9964]],\n",
       "\n",
       "        [[0.0060]],\n",
       "\n",
       "        [[0.0056]],\n",
       "\n",
       "        [[0.9968]],\n",
       "\n",
       "        [[0.9923]],\n",
       "\n",
       "        [[0.9977]],\n",
       "\n",
       "        [[0.0056]],\n",
       "\n",
       "        [[0.0044]],\n",
       "\n",
       "        [[0.9977]],\n",
       "\n",
       "        [[0.9987]],\n",
       "\n",
       "        [[0.9974]],\n",
       "\n",
       "        [[0.9945]],\n",
       "\n",
       "        [[0.9953]],\n",
       "\n",
       "        [[0.9974]],\n",
       "\n",
       "        [[0.0108]],\n",
       "\n",
       "        [[0.0072]],\n",
       "\n",
       "        [[0.9969]],\n",
       "\n",
       "        [[0.9969]],\n",
       "\n",
       "        [[0.9985]],\n",
       "\n",
       "        [[0.9981]],\n",
       "\n",
       "        [[0.9982]],\n",
       "\n",
       "        [[0.9963]],\n",
       "\n",
       "        [[0.9994]],\n",
       "\n",
       "        [[0.0038]],\n",
       "\n",
       "        [[0.9940]],\n",
       "\n",
       "        [[0.9968]],\n",
       "\n",
       "        [[0.0248]],\n",
       "\n",
       "        [[0.9970]],\n",
       "\n",
       "        [[0.9923]],\n",
       "\n",
       "        [[0.9983]],\n",
       "\n",
       "        [[0.0065]],\n",
       "\n",
       "        [[0.9977]],\n",
       "\n",
       "        [[0.9972]],\n",
       "\n",
       "        [[0.0049]],\n",
       "\n",
       "        [[0.9983]],\n",
       "\n",
       "        [[0.9979]],\n",
       "\n",
       "        [[0.9957]],\n",
       "\n",
       "        [[0.0105]],\n",
       "\n",
       "        [[0.9963]],\n",
       "\n",
       "        [[0.9949]],\n",
       "\n",
       "        [[0.0061]],\n",
       "\n",
       "        [[0.0047]],\n",
       "\n",
       "        [[0.9957]],\n",
       "\n",
       "        [[0.0102]],\n",
       "\n",
       "        [[0.9970]],\n",
       "\n",
       "        [[0.9981]],\n",
       "\n",
       "        [[0.3927]],\n",
       "\n",
       "        [[0.8812]],\n",
       "\n",
       "        [[0.6703]],\n",
       "\n",
       "        [[0.9689]],\n",
       "\n",
       "        [[0.8285]],\n",
       "\n",
       "        [[0.7597]],\n",
       "\n",
       "        [[0.7357]],\n",
       "\n",
       "        [[0.7800]],\n",
       "\n",
       "        [[0.6864]],\n",
       "\n",
       "        [[0.8599]],\n",
       "\n",
       "        [[0.9525]],\n",
       "\n",
       "        [[0.8916]],\n",
       "\n",
       "        [[0.9734]],\n",
       "\n",
       "        [[0.8590]],\n",
       "\n",
       "        [[0.9520]],\n",
       "\n",
       "        [[0.9455]],\n",
       "\n",
       "        [[0.6068]],\n",
       "\n",
       "        [[0.9324]],\n",
       "\n",
       "        [[0.7814]],\n",
       "\n",
       "        [[0.9548]],\n",
       "\n",
       "        [[0.9527]],\n",
       "\n",
       "        [[0.9768]],\n",
       "\n",
       "        [[0.8809]],\n",
       "\n",
       "        [[0.7739]],\n",
       "\n",
       "        [[0.9301]],\n",
       "\n",
       "        [[0.8352]],\n",
       "\n",
       "        [[0.7907]],\n",
       "\n",
       "        [[0.7037]],\n",
       "\n",
       "        [[0.8564]],\n",
       "\n",
       "        [[0.8016]],\n",
       "\n",
       "        [[0.8014]],\n",
       "\n",
       "        [[0.9017]],\n",
       "\n",
       "        [[0.9192]],\n",
       "\n",
       "        [[0.8032]],\n",
       "\n",
       "        [[0.8319]],\n",
       "\n",
       "        [[0.8994]],\n",
       "\n",
       "        [[0.9705]],\n",
       "\n",
       "        [[0.5984]],\n",
       "\n",
       "        [[0.9239]],\n",
       "\n",
       "        [[0.8573]],\n",
       "\n",
       "        [[0.9528]],\n",
       "\n",
       "        [[0.9716]],\n",
       "\n",
       "        [[0.9197]],\n",
       "\n",
       "        [[0.6132]],\n",
       "\n",
       "        [[0.7350]],\n",
       "\n",
       "        [[0.9764]]], device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = data_torch.cuda()\n",
    "net(input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of the secure environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "def connect_to_workers(n_workers):\n",
    "    return [\n",
    "        sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
    "        for i in range(n_workers)\n",
    "    ]\n",
    "\n",
    "def connect_to_crypto_provider():\n",
    "    return sy.VirtualWorker(hook, id=\"crypto_provider\")\n",
    "\n",
    "workers = connect_to_workers(n_workers=2)\n",
    "crypto_provider = connect_to_crypto_provider()\n",
    "\n",
    "def get_private_data_loaders(precision_fractional, workers, crypto_provider):\n",
    "    \n",
    "    def secret_share(tensor): #Transforms to fixed precision and secret share a tensor\n",
    "        return (\n",
    "            tensor\n",
    "            .fix_precision(precision_fractional=precision_fractional)\n",
    "            .share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "        )\n",
    "    \n",
    "    private_train_loader = [\n",
    "        (secret_share(torch.Tensor(x_train[i*5:i*5+5])), secret_share(torch.Tensor(y_train[i*5:i*5+5])))\n",
    "        for i in range (n_train_items)\n",
    "        if i < n_train_items / 5\n",
    "    ]\n",
    "    \n",
    "    private_test_loader = [\n",
    "        (secret_share(torch.Tensor(x_test[i*5:i*5+5])), secret_share(torch.Tensor(y_test[i*5:i*5+5])))\n",
    "        for i in range (n_train_items)\n",
    "        if i < n_train_items / 5\n",
    "    ]\n",
    "    return private_train_loader, private_test_loader\n",
    "    \n",
    "    \n",
    "private_train_loader, private_test_loader = get_private_data_loaders(\n",
    "    precision_fractional=3,\n",
    "    workers=workers,\n",
    "    crypto_provider=crypto_provider\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model for encrypted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res1d(nn.Module):\n",
    "    # the conv layers\n",
    "    def __init__(self, inSize, outSize, kernel=(3,), strides=1,):\n",
    "        super(Res1d, self).__init__()\n",
    "        self.inSize = inSize\n",
    "        self.outSize = outSize\n",
    "        # hard-coded to do the padding correctly\n",
    "        if inSize in (16,64,128,512) and strides is 2:\n",
    "            pding = 0\n",
    "        else:\n",
    "            pding = 1\n",
    "        self.l1 = nn.Conv1d(inSize, outSize, kernel, stride=strides, padding=pding, bias=False)\n",
    "        self.l2 = nn.Identity()\n",
    "        \n",
    "        if strides > 1 or inSize != outSize:\n",
    "            if strides > 1:\n",
    "                self.r1 = nn.Identity()\n",
    "                self.r2 = nn.AvgPool1d(strides)\n",
    "            else:\n",
    "                self.r1 = None\n",
    "                self.r2 = None\n",
    "            self.r3 = nn.Conv1d(inSize, outSize, 1, bias=False)\n",
    "            self.r4 = nn.Identity()\n",
    "            \n",
    "    def forward(self, x):\n",
    "        l = x\n",
    "        l = self.l1(l)\n",
    "        l = self.l2(l)\n",
    "        \n",
    "        if self.r1 is not None:\n",
    "            r = self.r1(x)\n",
    "            r = self.r2(r)\n",
    "            r = self.r3(r)\n",
    "            r = self.r4(r)\n",
    "        else:\n",
    "            r = self.r3(x)\n",
    "            r = self.r4(r)\n",
    "            \n",
    "        x = l + r\n",
    "        print(\"forwarding: \", self.inSize, self.outSize)\n",
    "        return F.relu(x)\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(dim, 64)\n",
    "        self.l2 = F.relu\n",
    "        \n",
    "        self.r1 = Res1d(1, 4, 3)\n",
    "        \n",
    "        self.r2 = Res1d(4, 8, 3)\n",
    "        self.r3 = Res1d(8, 8, 3, strides=2)\n",
    "        \n",
    "        self.r4 = Res1d(8, 16, 3)\n",
    "        self.r5 = Res1d(16, 16, 3, strides=2)\n",
    "        \n",
    "        self.r6 = Res1d(16, 32, 3)\n",
    "        self.r7 = Res1d(32, 32, 3, strides=2)\n",
    "        \n",
    "        self.r8 = Res1d(32, 64, 3)\n",
    "        self.r9 = Res1d(64, 64, 3, strides=2)\n",
    "        \n",
    "        self.r10 = Res1d(64, 128, 3)\n",
    "        self.r11 = Res1d(128, 128, 3, strides=2)\n",
    "        \n",
    "        self.r12 = Res1d(128, 256, 3)\n",
    "        self.r13 = Res1d(256, 256, 3, strides=2)\n",
    "        \n",
    "        self.r14 = Res1d(256, 512, 3)\n",
    "        self.r15 = Res1d(512, 512, 3, strides=2)\n",
    "        \n",
    "        self.r16 = Res1d(512, 1024, 3)\n",
    "        self.r17 = Res1d(1024, 1024, 3, strides=2)\n",
    "        \n",
    "        # size is by experiment and hardcode\n",
    "        self.lastLinear = nn.Linear(50240,32)\n",
    "        self.lastRelu = F.relu\n",
    "        self.lastAgg = nn.Linear(32,1)\n",
    "        self.lastSigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):           \n",
    "        # shape is (batch, channel, time)\n",
    "        l = x\n",
    "        l = x.view(x.shape[0],-1)\n",
    "        l = self.l1(l)\n",
    "        l = self.l2(l)\n",
    "\n",
    "        # conv layers should operate on time\n",
    "        r = x\n",
    "        r = self.r1(r)\n",
    "        r = self.r4(self.r3(self.r2(r)))\n",
    "        r = self.r8(self.r7(self.r6(self.r5(r))))\n",
    "        r = self.r12(self.r11(self.r10(self.r9(r))))\n",
    "        r = self.r16(self.r15(self.r14(self.r13(r))))\n",
    "        r = self.r17(r)\n",
    "        \n",
    "        # flatten l\n",
    "        r = r.view(x.shape[0],-1)\n",
    "        l = l.view(x.shape[0],-1)\n",
    "        y = torch.cat((l,r),dim=1)\n",
    "        y = self.lastLinear(y)\n",
    "        y = self.lastRelu(y)\n",
    "        y = self.lastAgg(y)\n",
    "        y = self.lastSigmoid(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Private Training of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, private_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(private_train_loader): # <-- now it is a private dataset\n",
    "        print('training...')\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        batch_size = output.shape[0]\n",
    "        loss = ((output - target)**2).sum().refresh()/batch_size\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "#Although the aim of this project was to provide private training, we prepared code for testing as well\n",
    "def test(model, private_test_loader):\n",
    "    print('testing...')\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in private_test_loader:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "    correct = correct.get().float_precision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "training...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n",
      "testing...\n",
      "forwarding:  1 4\n",
      "forwarding:  4 8\n",
      "forwarding:  8 8\n",
      "forwarding:  8 16\n",
      "forwarding:  16 16\n",
      "forwarding:  16 32\n",
      "forwarding:  32 32\n",
      "forwarding:  32 64\n",
      "forwarding:  64 64\n",
      "forwarding:  64 128\n",
      "forwarding:  128 128\n",
      "forwarding:  128 256\n",
      "forwarding:  256 256\n",
      "forwarding:  256 512\n",
      "forwarding:  512 512\n",
      "forwarding:  512 1024\n",
      "forwarding:  1024 1024\n"
     ]
    },
        {
     "ename": "RuntimeError",
     "evalue": "shape '[1]' is invalid for input of size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b36cc8ffdaa1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate_test_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-118cd7fa07be>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(model, private_test_loader)\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorrect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat_precision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0;31m# Send the new command to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m                 \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_syft_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;31m# Send it to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# Put back SyftTensor on the tensors found in the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_syft_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m             \u001b[0;31m# Send it to the appropriate class and get the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0;31m# Put back SyftTensor on the tensors found in the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/frameworks/torch/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_attr\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnew_self\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m             \u001b[0;31m# Put back AdditiveSharingTensor on the tensors found in the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_pointer_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0mcommand\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0;31m# For inplace methods, just directly return self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, recipient, message, return_ids)\u001b[0m\n\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mOperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrecipient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mResponseSignatureError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mret_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36msend_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;31m# Step 2: send the message and wait for a response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m         \u001b[0mbin_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbin_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# Step 3: deserialize the response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_send_msg\u001b[0;34m(self, message, location)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mVirtualWorker\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseWorker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFederatedClient\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_send_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBaseWorker\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlocation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/workers/virtual.py\u001b[0m in \u001b[0;36m_recv_msg\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mrecv_msg\u001b[0;34m(self, bin_message)\u001b[0m\n\u001b[1;32m    290\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"worker {self} received {sy.codes.code2MSGTYPE[msg_type]} {contents}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;31m# Step 1: route message to appropriate function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_message_router\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmsg_type\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;31m# Step 2: Serialize the message to simple python objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/workers/base.py\u001b[0m in \u001b[0;36mexecute_command\u001b[0;34m(self, message)\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_self\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                     \u001b[0;31m# TODO Andrew thinks this is gross, please fix. Instead need to properly deserialize strings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                     \u001b[0;31m# we can make some errors more descriptive with this method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mroute_method_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# means that there is a wrapper to remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_native_method\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m                     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                     \u001b[0;31m# we can make some errors more descriptive with this method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1]' is invalid for input of size 5"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model = model.fix_precision().share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "optimizer = optimizer.fix_precision() \n",
    "for epoch in range(1, 2):\n",
    "    train(model, private_train_loader, optimizer, epoch)\n",
    "    test(model, private_test_loader)"
   ]
  },
 
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
