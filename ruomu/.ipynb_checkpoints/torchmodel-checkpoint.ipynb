{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1.0\n"
     ]
    }
   ],
   "source": [
    "import torch; print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST('../data', train=True, download=True, transform=transformation),\n",
    "        batch_size=7\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 14\n",
    "        self.test_batch_size = 14\n",
    "        self.epochs = 10\n",
    "        self.lr = 0.001\n",
    "        self.seed = 1\n",
    "        self.log_interval = 1 # Log info at each batch\n",
    "        self.precision_fractional = 3\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "_ = torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/h/houruomu/.local/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "WARNING:tf_encrypted:Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/home/h/houruomu/.local/lib/python3.6/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_1.14.0.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/h/houruomu/.local/lib/python3.6/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/h/houruomu/.local/lib/python3.6/site-packages/tf_encrypted/session.py:26: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "def connect_to_workers(n_workers):\n",
    "    return [\n",
    "        sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
    "        for i in range(n_workers)\n",
    "    ]\n",
    "def connect_to_crypto_provider():\n",
    "    return sy.VirtualWorker(hook, id=\"crypto_provider\")\n",
    "\n",
    "workers = connect_to_workers(n_workers=2)\n",
    "crypto_provider = connect_to_crypto_provider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def getSamples(filename):\n",
    "    data = pd.read_csv(filename, sep='\\t')\n",
    "    return data.values[:,1:].transpose()\n",
    "\n",
    "dim = 12634\n",
    "data1 = getSamples(\"GSE2034-Normal-train.txt\")\n",
    "data2 = getSamples(\"GSE2034-Tumor-train.txt\")\n",
    "\n",
    "data1Label = np.zeros(len(data1)).reshape((-1, 1))\n",
    "data2Label = np.ones(len(data2)).reshape((-1, 1))\n",
    "x = np.concatenate((data1, data2))\n",
    "y = np.concatenate((data1Label, data2Label))\n",
    "# https://stackoverflow.com/questions/43229034/randomly-shuffle-data-and-labels-from-different-files-in-the-same-order\n",
    "# shuffle the data\n",
    "idx = np.random.permutation(len(x))\n",
    "x,y = x[idx], y[idx]\n",
    "\n",
    "z = np.concatenate((x, y), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We don't use the whole dataset for efficiency purpose, but feel free to increase these numbers\n",
    "n_train_items = 181\n",
    "n_test_items = 46\n",
    "\n",
    "# partition the data into training data and test data\n",
    "x_train = x[:n_train_items]\n",
    "y_train = y[:n_train_items]\n",
    "z_train = z[:n_train_items]\n",
    "\n",
    "x_test = x[n_train_items:]\n",
    "y_test = y[n_train_items:]    \n",
    "z_test = z[n_train_items:]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(46, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((-1,1,dim))\n",
    "y_train = y_train.reshape((-1,1))\n",
    "def get_private_data_loaders(precision_fractional, workers, crypto_provider):\n",
    "    \n",
    "    def one_hot_of(index_tensor):#Transforms to one hot tensor\n",
    "     \n",
    "        onehot_tensor = torch.zeros(*index_tensor.shape, 2) # 2 Output classes\n",
    "        onehot_tensor = onehot_tensor.scatter(1, index_tensor.view(-1, 1), 1)\n",
    "        return onehot_tensor\n",
    "        \n",
    "    def secret_share(tensor): #Transforms to fixed precision and secret share a tensor\n",
    "\n",
    "        return (\n",
    "            tensor\n",
    "            .fix_precision(precision_fractional=precision_fractional)\n",
    "            .share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "        )\n",
    "    \n",
    "    #transformation = transforms.Compose([\n",
    "    #    transforms.ToTensor(),\n",
    "    #    transforms.Normalize((0.1307,), (0.3081,))\n",
    "    #])\n",
    "    \n",
    "    \n",
    "    \n",
    "    #train_loader = torch.utils.data.DataLoader(\n",
    "    #    torch.Tensor(x_train),\n",
    "    #    batch_size=args.batch_size\n",
    "    #)\n",
    "\n",
    "    private_train_loader = [\n",
    "        (secret_share(torch.Tensor(x_train[i])), secret_share(one_hot_of(torch.LongTensor(y_train[i]))))\n",
    "        for i in range (n_train_items)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "    \n",
    "    #test_loader = torch.utils.data.DataLoader(\n",
    "    #    torch.Tensor(z_test),\n",
    "    #    batch_size=args.test_batch_size\n",
    "    #)\n",
    "        \n",
    "    private_test_loader = [\n",
    "        (secret_share(torch.Tensor(x_test[i])), secret_share(torch.Tensor(y_test[i])))\n",
    "        for i in range (n_test_items)\n",
    "        if i < n_test_items / args.test_batch_size\n",
    "    ]\n",
    "    return private_train_loader, private_test_loader\n",
    "    \n",
    "    \n",
    "private_train_loader, private_test_loader = get_private_data_loaders(\n",
    "    precision_fractional=args.precision_fractional,\n",
    "    workers=workers,\n",
    "    crypto_provider=crypto_provider\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Res1d(nn.Module):\n",
    "    # the conv layers\n",
    "    def __init__(self, inSize, outSize, kernel=(3,), strides=1,):\n",
    "        super(Res1d, self).__init__()\n",
    "        # hard-coded to do the padding correctly\n",
    "        if inSize in (16,64,128,512) and strides is 2:\n",
    "            pding = 0\n",
    "        else:\n",
    "            pding = 1\n",
    "        self.l1 = nn.Conv1d(inSize, outSize, kernel, stride=strides, padding=pding, bias=False)\n",
    "        self.l2 = nn.InstanceNorm1d(outSize)\n",
    "        \n",
    "        if strides > 1 or inSize != outSize:\n",
    "            if strides > 1:\n",
    "                self.r1 = nn.Identity()\n",
    "                self.r2 = nn.AvgPool1d(strides)\n",
    "            else:\n",
    "                self.r1 = None\n",
    "                self.r2 = None\n",
    "            self.r3 = nn.Conv1d(inSize, outSize, 1, bias=False)\n",
    "            self.r4 = nn.InstanceNorm1d(outSize)\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l = x\n",
    "        l = self.l1(l)\n",
    "        l = self.l2(l)\n",
    "        \n",
    "        if self.r1 is not None:\n",
    "            r = self.r1(x)\n",
    "            r = self.r2(r)\n",
    "            r = self.r3(r)\n",
    "            r = self.r4(r)\n",
    "        else:\n",
    "            r = self.r3(x)\n",
    "            r = self.r4(r)\n",
    "            \n",
    "        x = l + r\n",
    "        return self.relu(x)\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(dim, 64)\n",
    "        self.l2 = nn.ReLU()\n",
    "        \n",
    "        self.r1 = Res1d(1, 4, 3)\n",
    "        \n",
    "        self.r2 = Res1d(4, 8, 3)\n",
    "        self.r3 = Res1d(8, 8, 3, strides=2)\n",
    "        \n",
    "        self.r4 = Res1d(8, 16, 3)\n",
    "        self.r5 = Res1d(16, 16, 3, strides=2)\n",
    "        \n",
    "        self.r6 = Res1d(16, 32, 3)\n",
    "        self.r7 = Res1d(32, 32, 3, strides=2)\n",
    "        \n",
    "        self.r8 = Res1d(32, 64, 3)\n",
    "        self.r9 = Res1d(64, 64, 3, strides=2)\n",
    "        \n",
    "        self.r10 = Res1d(64, 128, 3)\n",
    "        self.r11 = Res1d(128, 128, 3, strides=2)\n",
    "        \n",
    "        self.r12 = Res1d(128, 256, 3)\n",
    "        self.r13 = Res1d(256, 256, 3, strides=2)\n",
    "        \n",
    "        self.r14 = Res1d(256, 512, 3)\n",
    "        self.r15 = Res1d(512, 512, 3, strides=2)\n",
    "        \n",
    "        self.r16 = Res1d(512, 1024, 3)\n",
    "        self.r17 = Res1d(1024, 1024, 3, strides=2)\n",
    "        \n",
    "        # size is by experiment and hardcode\n",
    "        self.lastLinear = nn.Linear(50240,32)\n",
    "        self.lastRelu = nn.ReLU()\n",
    "        self.lastAgg = nn.Linear(32,1)\n",
    "        self.lastSigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):           \n",
    "        # shape is (batch, channel, time)\n",
    "        l = x\n",
    "        l = self.l1(l)\n",
    "        l = self.l2(l)\n",
    "\n",
    "        # conv layers should operate on time\n",
    "        r = x\n",
    "        r = self.r4(self.r3(self.r2(self.r1(r))))\n",
    "        r = self.r8(self.r7(self.r6(self.r5(r))))\n",
    "        r = self.r12(self.r11(self.r10(self.r9(r))))\n",
    "        r = self.r16(self.r15(self.r14(self.r13(r))))\n",
    "        r = self.r17(r)\n",
    "        \n",
    "        # flatten l\n",
    "        r = r.view(x.shape[0],1, -1)\n",
    "        y = torch.cat((l,r),-1)\n",
    "        y = self.lastLinear(y)\n",
    "        y = self.lastRelu(y)\n",
    "        y = self.lastAgg(y)\n",
    "        y = self.lastSigmoid(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, private_train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(private_train_loader): # <-- now it is a private dataset\n",
    "        start_time = time.time()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        inputs = data.view([-1, 1, dim])\n",
    "        labels = target.view([-1, 1])\n",
    "#         inputs = data\n",
    "#         labels = target\n",
    "        output = model(inputs)\n",
    "        \n",
    "        # loss = F.nll_loss(output, target)  <-- not possible here\n",
    "        batch_size = output.shape[0]\n",
    "        loss = ((output - labels)**2).sum().refresh()/batch_size\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get().float_precision()\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.3f}s'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(private_train_loader) * args.batch_size,\n",
    "                100. * batch_idx / len(private_train_loader), loss.item(), time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, private_test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in private_test_loader:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target.view_as(pred)).sum()\n",
    "\n",
    "    correct = correct.get().float_precision()\n",
    "    print('\\nTest set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct.item(), len(private_test_loader)* args.test_batch_size,\n",
    "        100. * correct.item() / (len(private_test_loader) * args.test_batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\")\n",
    "#net = Net().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_test = torch.randn(80, 1, dim).float().cuda()\n",
    "#out = net(input_test).cuda()\n",
    "#out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "relu(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36munwrap_args_from_function\u001b[0;34m(attr, args, kwargs, return_args_type)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# TODO rename registry or use another one than for methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mhook_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook_method_args_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0mget_tensor_type_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tensor_type_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'torch.nn.functional.relu'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36mbuild_get_tensor_type\u001b[0;34m(rules, layer)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPureFrameworkTensorFoundError\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    302\u001b[0m             new_args, new_kwargs, new_type, args_type = hook_args.unwrap_args_from_function(\n\u001b[0;32m--> 303\u001b[0;31m                 \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_args_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36munwrap_args_from_function\u001b[0;34m(attr, args, kwargs, return_args_type)\u001b[0m\n\u001b[1;32m    169\u001b[0m         args_hook_function, get_tensor_type_function = build_unwrap_args_from_function(\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36mbuild_unwrap_args_from_function\u001b[0;34m(args, return_tuple)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# tensor found in the args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mget_tensor_type_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_get_tensor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0margs_hook_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_tensor_type_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36mbuild_get_tensor_type\u001b[0;34m(rules, layer)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;31m# the un-hooked (so native) function which is perfect in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPureFrameworkTensorFoundError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPureFrameworkTensorFoundError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36munwrap_args_from_function\u001b[0;34m(attr, args, kwargs, return_args_type)\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;31m# TODO rename registry or use another one than for methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mhook_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook_method_args_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0mget_tensor_type_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_tensor_type_functions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'torch.relu'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36mbuild_get_tensor_type\u001b[0;34m(rules, layer)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mlambdas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mPureFrameworkTensorFoundError\u001b[0m             Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    302\u001b[0m             new_args, new_kwargs, new_type, args_type = hook_args.unwrap_args_from_function(\n\u001b[0;32m--> 303\u001b[0;31m                 \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_args_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m             )\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36munwrap_args_from_function\u001b[0;34m(attr, args, kwargs, return_args_type)\u001b[0m\n\u001b[1;32m    169\u001b[0m         args_hook_function, get_tensor_type_function = build_unwrap_args_from_function(\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36mbuild_unwrap_args_from_function\u001b[0;34m(args, return_tuple)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# tensor found in the args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     \u001b[0mget_tensor_type_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_get_tensor_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0margs_hook_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_tensor_type_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook_args.py\u001b[0m in \u001b[0;36mbuild_get_tensor_type\u001b[0;34m(rules, layer)\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;31m# the un-hooked (so native) function which is perfect in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPureFrameworkTensorFoundError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPureFrameworkTensorFoundError\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e79443039c9b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate_test_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-d5b68794de81>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, private_train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#         inputs = data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#         labels = target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m# loss = F.nll_loss(output, target)  <-- not possible here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-fd33264cbaaf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;31m# conv layers should operate on time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0mhandle_func_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msyft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_func_command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_func_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# in the execute_command function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 943\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    944\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/generic/frameworks/hook/hook.py\u001b[0m in \u001b[0;36moverloaded_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0mhandle_func_command\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msyft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_func_command\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle_func_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/syft/frameworks/torch/tensors/interpreters/native.py\u001b[0m in \u001b[0;36mhandle_func_command\u001b[0;34m(cls, command)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;31m# in the execute_command function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m                 \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: relu(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model = model.fix_precision().share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "optimizer = optimizer.fix_precision() \n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, private_train_loader, optimizer, epoch)\n",
    "    test(args, model, private_test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encrypted Training Ends Here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data into (batch, channel = 1, length=dim)\n",
    "data_torch = torch.from_numpy(x).view([-1, 1, dim]).float()\n",
    "label_torch = torch.from_numpy(y).view([-1,1,1]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "net=Net()\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy \n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 % Trained loss =  0.7347368001937866\n",
      "1 % Trained loss =  0.677176296710968\n",
      "2 % Trained loss =  0.6332848072052002\n",
      "3 % Trained loss =  0.682956337928772\n",
      "4 % Trained loss =  0.5419864654541016\n",
      "5 % Trained loss =  0.6018826365470886\n",
      "6 % Trained loss =  0.5016396045684814\n",
      "7 % Trained loss =  0.5087392330169678\n",
      "8 % Trained loss =  0.47488686442375183\n",
      "9 % Trained loss =  0.5279948711395264\n",
      "10 % Trained loss =  0.44033798575401306\n",
      "11 % Trained loss =  0.4272150695323944\n",
      "12 % Trained loss =  0.35304442048072815\n",
      "13 % Trained loss =  0.36744973063468933\n",
      "14 % Trained loss =  0.3320218622684479\n",
      "15 % Trained loss =  0.28930553793907166\n",
      "16 % Trained loss =  0.4309445917606354\n",
      "17 % Trained loss =  0.43173375725746155\n",
      "18 % Trained loss =  0.2578074336051941\n",
      "19 % Trained loss =  0.33714187145233154\n",
      "20 % Trained loss =  0.2857415974140167\n",
      "21 % Trained loss =  0.18926239013671875\n",
      "22 % Trained loss =  0.16352412104606628\n",
      "23 % Trained loss =  0.3497416377067566\n",
      "24 % Trained loss =  0.4228338301181793\n",
      "25 % Trained loss =  0.16261693835258484\n",
      "26 % Trained loss =  0.2158360630273819\n",
      "27 % Trained loss =  0.2361796349287033\n",
      "28 % Trained loss =  0.19194674491882324\n",
      "29 % Trained loss =  0.10749049484729767\n",
      "30 % Trained loss =  0.07436687499284744\n",
      "31 % Trained loss =  0.11337651312351227\n",
      "32 % Trained loss =  0.13316962122917175\n",
      "33 % Trained loss =  0.09814958274364471\n",
      "34 % Trained loss =  0.07967619597911835\n",
      "35 % Trained loss =  0.09171538054943085\n",
      "36 % Trained loss =  0.06206701695919037\n",
      "37 % Trained loss =  0.06649822741746902\n",
      "38 % Trained loss =  0.04253249987959862\n",
      "39 % Trained loss =  0.06340382993221283\n",
      "40 % Trained loss =  0.044763341546058655\n",
      "41 % Trained loss =  0.03197683393955231\n",
      "42 % Trained loss =  0.028555670753121376\n",
      "43 % Trained loss =  0.02503983862698078\n",
      "44 % Trained loss =  0.025652820244431496\n",
      "45 % Trained loss =  0.02041822113096714\n",
      "46 % Trained loss =  0.02783166617155075\n",
      "47 % Trained loss =  0.025448622182011604\n",
      "48 % Trained loss =  0.011860916391015053\n",
      "49 % Trained loss =  0.015963921323418617\n",
      "50 % Trained loss =  0.02216002158820629\n",
      "51 % Trained loss =  0.008667472749948502\n",
      "52 % Trained loss =  0.013241037726402283\n",
      "53 % Trained loss =  0.011050534434616566\n",
      "54 % Trained loss =  0.010453706607222557\n",
      "55 % Trained loss =  0.008175517432391644\n",
      "56 % Trained loss =  0.0084135252982378\n",
      "57 % Trained loss =  0.009713998064398766\n",
      "58 % Trained loss =  0.009951979853212833\n",
      "59 % Trained loss =  0.009545586071908474\n",
      "60 % Trained loss =  0.0073975506238639355\n",
      "61 % Trained loss =  0.007769720628857613\n",
      "62 % Trained loss =  0.007700668182224035\n",
      "63 % Trained loss =  0.006233186926692724\n",
      "64 % Trained loss =  0.0061261472292244434\n",
      "65 % Trained loss =  0.00688672112300992\n",
      "66 % Trained loss =  0.007315078284591436\n",
      "67 % Trained loss =  0.006818787194788456\n",
      "68 % Trained loss =  0.004877370782196522\n",
      "69 % Trained loss =  0.004256655927747488\n",
      "70 % Trained loss =  0.0049120779149234295\n",
      "71 % Trained loss =  0.004169363994151354\n",
      "72 % Trained loss =  0.004565999377518892\n",
      "73 % Trained loss =  0.0059791202656924725\n",
      "74 % Trained loss =  0.003436818951740861\n",
      "75 % Trained loss =  0.004006888251751661\n",
      "76 % Trained loss =  0.005833650939166546\n",
      "77 % Trained loss =  0.00290444353595376\n",
      "78 % Trained loss =  0.005435059312731028\n",
      "79 % Trained loss =  0.003673739265650511\n",
      "80 % Trained loss =  0.003598438808694482\n",
      "81 % Trained loss =  0.0037503433413803577\n",
      "82 % Trained loss =  0.003946484066545963\n",
      "83 % Trained loss =  0.0038369116373360157\n",
      "84 % Trained loss =  0.0035272196400910616\n",
      "85 % Trained loss =  0.0036889000330120325\n",
      "86 % Trained loss =  0.0027298061177134514\n",
      "87 % Trained loss =  0.003726221388205886\n",
      "88 % Trained loss =  0.002804083051159978\n",
      "89 % Trained loss =  0.003080399241298437\n",
      "90 % Trained loss =  0.0029237254057079554\n",
      "91 % Trained loss =  0.002902774605900049\n",
      "92 % Trained loss =  0.0036875198129564524\n",
      "93 % Trained loss =  0.004059321712702513\n",
      "94 % Trained loss =  0.003114350838586688\n",
      "95 % Trained loss =  0.0033464462030678988\n",
      "96 % Trained loss =  0.0030134988483041525\n",
      "97 % Trained loss =  0.0032939324155449867\n",
      "98 % Trained loss =  0.0024112281389534473\n",
      "99 % Trained loss =  0.003167303977534175\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "test_inputs = torch.from_numpy(x_test).view([-1, 1, dim]).float()\n",
    "test_labels = torch.from_numpy(y_test).view([-1, 1]).float()\n",
    "for batch in range(100):  # loop over the dataset multiple times\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    indices = np.random.choice(len(x_train), size=(30))\n",
    "    inputs = x_train[indices]\n",
    "    labels = y_train[indices]\n",
    "    \n",
    "    inputs = torch.from_numpy(inputs).float()\n",
    "    labels = torch.from_numpy(labels).float()\n",
    "#     inputs = torch.from_numpy(inputs).view([-1, 1, dim]).float()\n",
    "#     labels = torch.from_numpy(labels).view([-1, 1]).float()\n",
    "    \n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(inputs).view([-1,1])\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(batch, \"% Trained\", \"loss = \", loss.item())\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[2.1642e-03]],\n",
       "\n",
       "        [[9.9946e-01]],\n",
       "\n",
       "        [[9.9937e-01]],\n",
       "\n",
       "        [[9.9949e-01]],\n",
       "\n",
       "        [[9.9772e-01]],\n",
       "\n",
       "        [[9.9855e-01]],\n",
       "\n",
       "        [[9.9930e-01]],\n",
       "\n",
       "        [[9.9884e-01]],\n",
       "\n",
       "        [[3.6327e-03]],\n",
       "\n",
       "        [[2.6941e-03]],\n",
       "\n",
       "        [[2.3771e-03]],\n",
       "\n",
       "        [[9.9801e-01]],\n",
       "\n",
       "        [[2.4945e-03]],\n",
       "\n",
       "        [[9.9907e-01]],\n",
       "\n",
       "        [[4.9189e-03]],\n",
       "\n",
       "        [[1.6478e-03]],\n",
       "\n",
       "        [[9.9898e-01]],\n",
       "\n",
       "        [[9.9872e-01]],\n",
       "\n",
       "        [[9.9893e-01]],\n",
       "\n",
       "        [[4.1863e-03]],\n",
       "\n",
       "        [[9.9802e-01]],\n",
       "\n",
       "        [[9.9917e-01]],\n",
       "\n",
       "        [[9.9854e-01]],\n",
       "\n",
       "        [[3.4620e-03]],\n",
       "\n",
       "        [[2.9419e-03]],\n",
       "\n",
       "        [[3.7487e-03]],\n",
       "\n",
       "        [[1.1803e-03]],\n",
       "\n",
       "        [[4.6078e-03]],\n",
       "\n",
       "        [[9.9912e-01]],\n",
       "\n",
       "        [[9.9887e-01]],\n",
       "\n",
       "        [[4.2135e-03]],\n",
       "\n",
       "        [[9.9907e-01]],\n",
       "\n",
       "        [[9.9903e-01]],\n",
       "\n",
       "        [[5.2026e-03]],\n",
       "\n",
       "        [[4.4798e-03]],\n",
       "\n",
       "        [[9.9760e-01]],\n",
       "\n",
       "        [[9.9961e-01]],\n",
       "\n",
       "        [[9.9619e-01]],\n",
       "\n",
       "        [[1.4576e-03]],\n",
       "\n",
       "        [[9.9898e-01]],\n",
       "\n",
       "        [[9.9874e-01]],\n",
       "\n",
       "        [[9.9802e-01]],\n",
       "\n",
       "        [[9.9888e-01]],\n",
       "\n",
       "        [[9.9871e-01]],\n",
       "\n",
       "        [[9.9947e-01]],\n",
       "\n",
       "        [[9.9752e-01]],\n",
       "\n",
       "        [[3.0367e-03]],\n",
       "\n",
       "        [[9.9780e-01]],\n",
       "\n",
       "        [[9.9901e-01]],\n",
       "\n",
       "        [[4.6647e-03]],\n",
       "\n",
       "        [[9.9844e-01]],\n",
       "\n",
       "        [[9.9940e-01]],\n",
       "\n",
       "        [[9.9873e-01]],\n",
       "\n",
       "        [[9.9820e-01]],\n",
       "\n",
       "        [[9.9908e-01]],\n",
       "\n",
       "        [[3.5587e-03]],\n",
       "\n",
       "        [[9.9841e-01]],\n",
       "\n",
       "        [[9.9907e-01]],\n",
       "\n",
       "        [[9.9847e-01]],\n",
       "\n",
       "        [[9.9832e-01]],\n",
       "\n",
       "        [[9.9912e-01]],\n",
       "\n",
       "        [[9.9878e-01]],\n",
       "\n",
       "        [[3.4786e-03]],\n",
       "\n",
       "        [[2.3101e-03]],\n",
       "\n",
       "        [[3.8995e-03]],\n",
       "\n",
       "        [[2.4285e-03]],\n",
       "\n",
       "        [[3.6627e-03]],\n",
       "\n",
       "        [[4.9703e-03]],\n",
       "\n",
       "        [[9.9825e-01]],\n",
       "\n",
       "        [[3.5658e-03]],\n",
       "\n",
       "        [[4.1270e-03]],\n",
       "\n",
       "        [[9.9807e-01]],\n",
       "\n",
       "        [[5.6748e-03]],\n",
       "\n",
       "        [[9.9871e-01]],\n",
       "\n",
       "        [[9.9703e-01]],\n",
       "\n",
       "        [[5.9028e-04]],\n",
       "\n",
       "        [[9.9839e-01]],\n",
       "\n",
       "        [[9.9858e-01]],\n",
       "\n",
       "        [[9.9898e-01]],\n",
       "\n",
       "        [[5.8522e-05]],\n",
       "\n",
       "        [[9.9859e-01]],\n",
       "\n",
       "        [[9.9740e-01]],\n",
       "\n",
       "        [[3.1784e-03]],\n",
       "\n",
       "        [[9.9827e-01]],\n",
       "\n",
       "        [[5.5616e-03]],\n",
       "\n",
       "        [[4.0377e-03]],\n",
       "\n",
       "        [[9.9931e-01]],\n",
       "\n",
       "        [[9.9717e-01]],\n",
       "\n",
       "        [[9.9861e-01]],\n",
       "\n",
       "        [[3.9056e-03]],\n",
       "\n",
       "        [[5.2726e-03]],\n",
       "\n",
       "        [[9.9810e-01]],\n",
       "\n",
       "        [[9.9703e-01]],\n",
       "\n",
       "        [[9.9825e-01]],\n",
       "\n",
       "        [[9.9860e-01]],\n",
       "\n",
       "        [[9.9877e-01]],\n",
       "\n",
       "        [[9.9905e-01]],\n",
       "\n",
       "        [[9.9618e-01]],\n",
       "\n",
       "        [[9.9899e-01]],\n",
       "\n",
       "        [[9.9833e-01]],\n",
       "\n",
       "        [[9.9766e-01]],\n",
       "\n",
       "        [[9.9904e-01]],\n",
       "\n",
       "        [[9.9783e-01]],\n",
       "\n",
       "        [[3.7278e-04]],\n",
       "\n",
       "        [[1.0771e-03]],\n",
       "\n",
       "        [[3.8243e-03]],\n",
       "\n",
       "        [[9.9733e-01]],\n",
       "\n",
       "        [[9.9804e-01]],\n",
       "\n",
       "        [[9.9822e-01]],\n",
       "\n",
       "        [[3.4242e-03]],\n",
       "\n",
       "        [[9.9825e-01]],\n",
       "\n",
       "        [[9.9957e-01]],\n",
       "\n",
       "        [[5.3784e-04]],\n",
       "\n",
       "        [[3.3220e-03]],\n",
       "\n",
       "        [[9.9848e-01]],\n",
       "\n",
       "        [[5.0220e-03]],\n",
       "\n",
       "        [[9.9653e-01]],\n",
       "\n",
       "        [[9.9815e-01]],\n",
       "\n",
       "        [[2.7039e-03]],\n",
       "\n",
       "        [[4.3178e-03]],\n",
       "\n",
       "        [[9.9703e-01]],\n",
       "\n",
       "        [[9.9869e-01]],\n",
       "\n",
       "        [[3.0022e-03]],\n",
       "\n",
       "        [[9.5006e-03]],\n",
       "\n",
       "        [[9.9686e-01]],\n",
       "\n",
       "        [[2.4006e-03]],\n",
       "\n",
       "        [[9.9829e-01]],\n",
       "\n",
       "        [[9.9907e-01]],\n",
       "\n",
       "        [[2.2656e-03]],\n",
       "\n",
       "        [[9.9820e-01]],\n",
       "\n",
       "        [[9.9797e-01]],\n",
       "\n",
       "        [[9.9857e-01]],\n",
       "\n",
       "        [[9.9818e-01]],\n",
       "\n",
       "        [[9.9845e-01]],\n",
       "\n",
       "        [[9.9809e-01]],\n",
       "\n",
       "        [[3.0890e-03]],\n",
       "\n",
       "        [[9.9879e-01]],\n",
       "\n",
       "        [[5.4613e-03]],\n",
       "\n",
       "        [[9.9898e-01]],\n",
       "\n",
       "        [[9.9828e-01]],\n",
       "\n",
       "        [[9.9766e-01]],\n",
       "\n",
       "        [[4.0180e-03]],\n",
       "\n",
       "        [[9.9888e-01]],\n",
       "\n",
       "        [[9.9946e-01]],\n",
       "\n",
       "        [[9.9850e-01]],\n",
       "\n",
       "        [[9.9916e-01]],\n",
       "\n",
       "        [[9.9849e-01]],\n",
       "\n",
       "        [[3.8062e-03]],\n",
       "\n",
       "        [[9.9859e-01]],\n",
       "\n",
       "        [[9.9682e-01]],\n",
       "\n",
       "        [[9.9905e-01]],\n",
       "\n",
       "        [[9.9896e-01]],\n",
       "\n",
       "        [[9.9837e-01]],\n",
       "\n",
       "        [[9.9780e-01]],\n",
       "\n",
       "        [[9.9842e-01]],\n",
       "\n",
       "        [[9.9882e-01]],\n",
       "\n",
       "        [[9.9890e-01]],\n",
       "\n",
       "        [[9.9849e-01]],\n",
       "\n",
       "        [[9.9787e-01]],\n",
       "\n",
       "        [[9.9943e-01]],\n",
       "\n",
       "        [[4.2669e-03]],\n",
       "\n",
       "        [[9.9918e-01]],\n",
       "\n",
       "        [[9.9933e-01]],\n",
       "\n",
       "        [[9.9664e-01]],\n",
       "\n",
       "        [[9.9935e-01]],\n",
       "\n",
       "        [[4.7928e-04]],\n",
       "\n",
       "        [[1.8174e-03]],\n",
       "\n",
       "        [[9.9775e-01]],\n",
       "\n",
       "        [[9.9823e-01]],\n",
       "\n",
       "        [[9.9856e-01]],\n",
       "\n",
       "        [[9.9924e-01]],\n",
       "\n",
       "        [[2.1203e-03]],\n",
       "\n",
       "        [[6.8782e-04]],\n",
       "\n",
       "        [[9.9777e-01]],\n",
       "\n",
       "        [[2.4859e-03]],\n",
       "\n",
       "        [[2.3244e-03]],\n",
       "\n",
       "        [[9.9953e-01]],\n",
       "\n",
       "        [[3.8670e-03]],\n",
       "\n",
       "        [[9.9718e-01]],\n",
       "\n",
       "        [[9.9992e-01]],\n",
       "\n",
       "        [[9.9939e-01]],\n",
       "\n",
       "        [[8.2686e-01]],\n",
       "\n",
       "        [[9.8507e-01]],\n",
       "\n",
       "        [[5.4645e-01]],\n",
       "\n",
       "        [[9.5841e-01]],\n",
       "\n",
       "        [[9.5688e-01]],\n",
       "\n",
       "        [[9.6197e-01]],\n",
       "\n",
       "        [[9.1133e-01]],\n",
       "\n",
       "        [[9.4140e-01]],\n",
       "\n",
       "        [[8.6396e-01]],\n",
       "\n",
       "        [[9.4885e-01]],\n",
       "\n",
       "        [[8.9227e-01]],\n",
       "\n",
       "        [[9.3855e-01]],\n",
       "\n",
       "        [[9.7626e-01]],\n",
       "\n",
       "        [[9.9210e-01]],\n",
       "\n",
       "        [[7.6277e-01]],\n",
       "\n",
       "        [[7.2815e-01]],\n",
       "\n",
       "        [[8.9351e-01]],\n",
       "\n",
       "        [[8.5408e-01]],\n",
       "\n",
       "        [[7.4280e-01]],\n",
       "\n",
       "        [[9.6505e-01]],\n",
       "\n",
       "        [[9.5423e-01]],\n",
       "\n",
       "        [[9.2836e-01]],\n",
       "\n",
       "        [[9.3214e-01]],\n",
       "\n",
       "        [[9.0158e-01]],\n",
       "\n",
       "        [[9.2549e-01]],\n",
       "\n",
       "        [[9.1235e-01]],\n",
       "\n",
       "        [[8.6610e-01]],\n",
       "\n",
       "        [[8.1952e-01]],\n",
       "\n",
       "        [[8.4497e-01]],\n",
       "\n",
       "        [[9.6561e-01]],\n",
       "\n",
       "        [[9.1886e-01]],\n",
       "\n",
       "        [[9.4978e-01]],\n",
       "\n",
       "        [[9.0744e-01]],\n",
       "\n",
       "        [[9.7039e-01]],\n",
       "\n",
       "        [[9.3818e-01]],\n",
       "\n",
       "        [[9.1155e-01]],\n",
       "\n",
       "        [[8.9354e-01]],\n",
       "\n",
       "        [[7.4475e-01]],\n",
       "\n",
       "        [[7.8135e-01]],\n",
       "\n",
       "        [[5.5813e-01]],\n",
       "\n",
       "        [[9.3521e-01]],\n",
       "\n",
       "        [[9.0183e-01]],\n",
       "\n",
       "        [[9.0392e-01]],\n",
       "\n",
       "        [[8.9161e-01]],\n",
       "\n",
       "        [[6.9209e-01]],\n",
       "\n",
       "        [[9.0335e-01]]], grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MPC learning\n",
    "net(data_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
