{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import numpy as np\n",
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Falling back to insecure randomness since the required custom op could not be found for the installed version of TensorFlow. Fix this by compiling custom ops. Missing file was '/home/souhail/.local/lib/python3.7/site-packages/tf_encrypted/operations/secure_random/secure_random_module_tf_2.0.0.so'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/souhail/.local/lib/python3.7/site-packages/tensorflow_core/python/compat/v2_compat.py:65: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import syft as sy\n",
    "\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "def connect_to_workers(n_workers):\n",
    "    return [\n",
    "        sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
    "        for i in range(n_workers)\n",
    "    ]\n",
    "def connect_to_crypto_provider():\n",
    "    return sy.VirtualWorker(hook, id=\"crypto_provider\")\n",
    "\n",
    "workers = connect_to_workers(n_workers=2)\n",
    "crypto_provider = connect_to_crypto_provider()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def getSamples(filename):\n",
    "    data = pd.read_csv(filename, sep='\\t')\n",
    "    return data.values[:,1:].transpose()\n",
    "\n",
    "data1 = getSamples(\"GSE2034-Normal-train.txt\")\n",
    "data2 = getSamples(\"GSE2034-Tumor-train.txt\")\n",
    "\n",
    "data1Label = np.zeros(len(data1)).reshape((-1, 1))\n",
    "data2Label = np.ones(len(data2)).reshape((-1, 1))\n",
    "x = np.concatenate((data1, data2))\n",
    "y = np.concatenate((data1Label, data2Label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "227"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We don't use the whole dataset for efficiency purpose, but feel free to increase these numbers\n",
    "n_train_items = 181\n",
    "n_test_items = 46\n",
    "\n",
    "def get_private_data_loaders(precision_fractional, workers, crypto_provider):\n",
    "    \n",
    "    def one_hot_of(index_tensor):#Transforms to one hot tensor\n",
    "     \n",
    "        onehot_tensor = torch.zeros(*index_tensor.shape, 2) # 2 Output classes\n",
    "        onehot_tensor = onehot_tensor.scatter(1, index_tensor.view(-1, 1), 1)\n",
    "        return onehot_tensor\n",
    "        \n",
    "    def secret_share(tensor): #Transforms to fixed precision and secret share a tensor\n",
    "\n",
    "        return (\n",
    "            tensor\n",
    "            .fix_precision(precision_fractional=precision_fractional)\n",
    "            .share(*workers, crypto_provider=crypto_provider, requires_grad=True)\n",
    "        )\n",
    "    \n",
    "    transformation = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "    ])\n",
    "    \n",
    "    private_train_loader = [\n",
    "        (secret_share(data), secret_share(one_hot_of(target)))\n",
    "        for i, (data, target) in enumerate(#Training Set)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "    \n",
    "    private_test_loader = [\n",
    "        (secret_share(data), secret_share(target.float()))\n",
    "        for i, (data, target) in enumerate(#Testing Set)\n",
    "        if i < n_test_items / args.test_batch_size\n",
    "    ]\n",
    "    \n",
    "    return private_train_loader, private_test_loader\n",
    "    \n",
    "    \n",
    "private_train_loader, private_test_loader = get_private_data_loaders(\n",
    "    precision_fractional=args.precision_fractional,\n",
    "    workers=workers,\n",
    "    crypto_provider=crypto_provider\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim = 12634\n",
    "\n",
    "class Res1d(nn.Module):\n",
    "    # the conv layers\n",
    "    def __init__(self, inSize, outSize, kernel=(3,), strides=1,):\n",
    "        super(Res1d, self).__init__()\n",
    "        # hard-coded to do the padding correctly\n",
    "        if inSize in (16,64,128,512) and strides is 2:\n",
    "            pding = 0\n",
    "        else:\n",
    "            pding = 1\n",
    "        self.l1 = nn.Conv1d(inSize, outSize, kernel, stride=strides, padding=pding, bias=False)\n",
    "        self.l2 = nn.InstanceNorm1d(outSize)\n",
    "        \n",
    "        if strides > 1 or inSize != outSize:\n",
    "            if strides > 1:\n",
    "                self.r1 = nn.Identity()\n",
    "                self.r2 = nn.AvgPool1d(strides)\n",
    "            else:\n",
    "                self.r1 = None\n",
    "                self.r2 = None\n",
    "            self.r3 = nn.Conv1d(inSize, outSize, 1, bias=False)\n",
    "            self.r4 = nn.InstanceNorm1d(outSize)\n",
    "    \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        l = self.l1(x)\n",
    "        l = self.l2(l)\n",
    "        \n",
    "        if self.r1 is not None:\n",
    "            r = self.r1(x)\n",
    "            r = self.r2(r)\n",
    "            r = self.r3(r)\n",
    "            r = self.r4(r)\n",
    "        else:\n",
    "            r = self.r3(x)\n",
    "            r = self.r4(r)\n",
    "            \n",
    "        x = l + r\n",
    "        return self.relu(x)\n",
    "    \n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.l1 = nn.Linear(dim, 64)\n",
    "        self.l2 = nn.ReLU()\n",
    "        \n",
    "        self.r1 = Res1d(1, 4, 3)\n",
    "        \n",
    "        self.r2 = Res1d(4, 8, 3)\n",
    "        self.r3 = Res1d(8, 8, 3, strides=2)\n",
    "        \n",
    "        self.r4 = Res1d(8, 16, 3)\n",
    "        self.r5 = Res1d(16, 16, 3, strides=2)\n",
    "        \n",
    "        self.r6 = Res1d(16, 32, 3)\n",
    "        self.r7 = Res1d(32, 32, 3, strides=2)\n",
    "        \n",
    "        self.r8 = Res1d(32, 64, 3)\n",
    "        self.r9 = Res1d(64, 64, 3, strides=2)\n",
    "        \n",
    "        self.r10 = Res1d(64, 128, 3)\n",
    "        self.r11 = Res1d(128, 128, 3, strides=2)\n",
    "        \n",
    "        self.r12 = Res1d(128, 256, 3)\n",
    "        self.r13 = Res1d(256, 256, 3, strides=2)\n",
    "        \n",
    "        self.r14 = Res1d(256, 512, 3)\n",
    "        self.r15 = Res1d(512, 512, 3, strides=2)\n",
    "        \n",
    "        self.r16 = Res1d(512, 1024, 3)\n",
    "        self.r17 = Res1d(1024, 1024, 3, strides=2)\n",
    "        \n",
    "        # size is by experiment and hardcode\n",
    "        self.lastLinear = nn.Linear(50240,32)\n",
    "        self.lastRelu = nn.ReLU()\n",
    "        self.lastAgg = nn.Linear(32,1)\n",
    "        self.lastSigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # shape is (batch, channel, time)\n",
    "        l = x\n",
    "        l = self.l2(self.l1(l))\n",
    "        \n",
    "        # conv layers should operate on time\n",
    "        r = x\n",
    "        r = self.r4(self.r3(self.r2(self.r1(r))))\n",
    "        r = self.r8(self.r7(self.r6(self.r5(r))))\n",
    "        r = self.r12(self.r11(self.r10(self.r9(r))))\n",
    "        r = self.r16(self.r15(self.r14(self.r13(r))))\n",
    "        r = self.r17(r)\n",
    "        \n",
    "        # flatten l\n",
    "        r = r.view(x.shape[0],1, -1)\n",
    "        y = torch.cat((l,r),-1)\n",
    "        y = self.lastLinear(y)\n",
    "        y = self.lastRelu(y)\n",
    "        y = self.lastAgg(y)\n",
    "        y = self.lastSigmoid(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "net = Net().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([80, 1, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_test = torch.randn(80, 1, dim).float().cuda()\n",
    "out = net(input_test).cuda()\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data into (batch, channel = 1, length=dim)\n",
    "data_torch = torch.from_numpy(x).view([-1, 1, dim]).float()\n",
    "label_torch = torch.from_numpy(y).view([-1,1,1]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.BCELoss() # Binary Cross Entropy \n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 % Trained\n",
      "2.0 % Trained\n",
      "3.0 % Trained\n",
      "4.0 % Trained\n",
      "5.0 % Trained\n",
      "6.0 % Trained\n",
      "7.0 % Trained\n",
      "8.0 % Trained\n",
      "9.0 % Trained\n",
      "10.0 % Trained\n",
      "11.0 % Trained\n",
      "12.0 % Trained\n",
      "13.0 % Trained\n",
      "14.0 % Trained\n",
      "15.0 % Trained\n",
      "16.0 % Trained\n",
      "17.0 % Trained\n",
      "18.0 % Trained\n",
      "19.0 % Trained\n",
      "20.0 % Trained\n",
      "21.0 % Trained\n",
      "22.0 % Trained\n",
      "23.0 % Trained\n",
      "24.0 % Trained\n",
      "25.0 % Trained\n",
      "26.0 % Trained\n",
      "27.0 % Trained\n",
      "28.0 % Trained\n",
      "29.0 % Trained\n",
      "30.0 % Trained\n",
      "31.0 % Trained\n",
      "32.0 % Trained\n",
      "33.0 % Trained\n",
      "34.0 % Trained\n",
      "35.0 % Trained\n",
      "36.0 % Trained\n",
      "37.0 % Trained\n",
      "38.0 % Trained\n",
      "39.0 % Trained\n",
      "40.0 % Trained\n",
      "41.0 % Trained\n",
      "42.0 % Trained\n",
      "43.0 % Trained\n",
      "44.0 % Trained\n",
      "45.0 % Trained\n",
      "46.0 % Trained\n",
      "47.0 % Trained\n",
      "48.0 % Trained\n",
      "49.0 % Trained\n",
      "50.0 % Trained\n",
      "51.0 % Trained\n",
      "52.0 % Trained\n",
      "53.0 % Trained\n",
      "54.0 % Trained\n",
      "55.0 % Trained\n",
      "56.0 % Trained\n",
      "57.0 % Trained\n",
      "58.0 % Trained\n",
      "59.0 % Trained\n",
      "60.0 % Trained\n",
      "61.0 % Trained\n",
      "62.0 % Trained\n",
      "63.0 % Trained\n",
      "64.0 % Trained\n",
      "65.0 % Trained\n",
      "66.0 % Trained\n",
      "67.0 % Trained\n",
      "68.0 % Trained\n",
      "69.0 % Trained\n",
      "70.0 % Trained\n",
      "71.0 % Trained\n",
      "72.0 % Trained\n",
      "73.0 % Trained\n",
      "74.0 % Trained\n",
      "75.0 % Trained\n",
      "76.0 % Trained\n",
      "77.0 % Trained\n",
      "78.0 % Trained\n",
      "79.0 % Trained\n",
      "80.0 % Trained\n",
      "81.0 % Trained\n",
      "82.0 % Trained\n",
      "83.0 % Trained\n",
      "84.0 % Trained\n",
      "85.0 % Trained\n",
      "86.0 % Trained\n",
      "87.0 % Trained\n",
      "88.0 % Trained\n",
      "89.0 % Trained\n",
      "90.0 % Trained\n",
      "91.0 % Trained\n",
      "92.0 % Trained\n",
      "93.0 % Trained\n",
      "94.0 % Trained\n",
      "95.0 % Trained\n",
      "96.0 % Trained\n",
      "97.0 % Trained\n",
      "98.0 % Trained\n",
      "99.0 % Trained\n",
      "100.0 % Trained\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for batch in range(1000):  # loop over the dataset multiple times\n",
    "    # get the inputs; data is a list of [inputs, labels]\n",
    "    indices = np.random.choice(len(x), size=(30))\n",
    "    inputs = x[indices]\n",
    "    labels = y[indices]\n",
    "    \n",
    "    inputs = torch.from_numpy(inputs).view([-1, 1, dim]).float().cuda()\n",
    "    labels = torch.from_numpy(labels).view([-1, 1]).float().cuda()\n",
    "    \n",
    "    # zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # forward + backward + optimize\n",
    "    outputs = net(inputs).view([-1,1]).cuda()\n",
    "    loss = criterion(outputs, labels).cuda()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if batch % 10 == 0:\n",
    "        print(batch / 10 + 1, \"% Trained\")\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (l1): Linear(in_features=12634, out_features=64, bias=True)\n",
       "  (l2): ReLU()\n",
       "  (r1): Res1d(\n",
       "    (l1): Conv1d(1, 4, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(1, 4, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(4, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r2): Res1d(\n",
       "    (l1): Conv1d(4, 8, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(4, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r3): Res1d(\n",
       "    (l1): Conv1d(8, 8, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r1): Identity()\n",
       "    (r2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (r3): Conv1d(8, 8, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(8, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r4): Res1d(\n",
       "    (l1): Conv1d(8, 16, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(8, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r5): Res1d(\n",
       "    (l1): Conv1d(16, 16, kernel_size=(3,), stride=(2,), bias=False)\n",
       "    (l2): InstanceNorm1d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r1): Identity()\n",
       "    (r2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (r3): Conv1d(16, 16, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r6): Res1d(\n",
       "    (l1): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(16, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r7): Res1d(\n",
       "    (l1): Conv1d(32, 32, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r1): Identity()\n",
       "    (r2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (r3): Conv1d(32, 32, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(32, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r8): Res1d(\n",
       "    (l1): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(32, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r9): Res1d(\n",
       "    (l1): Conv1d(64, 64, kernel_size=(3,), stride=(2,), bias=False)\n",
       "    (l2): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r1): Identity()\n",
       "    (r2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (r3): Conv1d(64, 64, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r10): Res1d(\n",
       "    (l1): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(64, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r11): Res1d(\n",
       "    (l1): Conv1d(128, 128, kernel_size=(3,), stride=(2,), bias=False)\n",
       "    (l2): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r1): Identity()\n",
       "    (r2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (r3): Conv1d(128, 128, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r12): Res1d(\n",
       "    (l1): Conv1d(128, 256, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(128, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r13): Res1d(\n",
       "    (l1): Conv1d(256, 256, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r1): Identity()\n",
       "    (r2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (r3): Conv1d(256, 256, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r14): Res1d(\n",
       "    (l1): Conv1d(256, 512, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(256, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r15): Res1d(\n",
       "    (l1): Conv1d(512, 512, kernel_size=(3,), stride=(2,), bias=False)\n",
       "    (l2): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r1): Identity()\n",
       "    (r2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (r3): Conv1d(512, 512, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r16): Res1d(\n",
       "    (l1): Conv1d(512, 1024, kernel_size=(3,), stride=(1,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r3): Conv1d(512, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (r17): Res1d(\n",
       "    (l1): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,), bias=False)\n",
       "    (l2): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (r1): Identity()\n",
       "    (r2): AvgPool1d(kernel_size=(2,), stride=(2,), padding=(0,))\n",
       "    (r3): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)\n",
       "    (r4): InstanceNorm1d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (lastLinear): Linear(in_features=50240, out_features=32, bias=True)\n",
       "  (lastRelu): ReLU()\n",
       "  (lastAgg): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (lastSigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MPC learning\n",
    "net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[4.0814e-04]],\n",
       "\n",
       "        [[5.1038e-04]],\n",
       "\n",
       "        [[3.8278e-04]],\n",
       "\n",
       "        [[5.1896e-04]],\n",
       "\n",
       "        [[2.9317e-04]],\n",
       "\n",
       "        [[5.0704e-04]],\n",
       "\n",
       "        [[3.5465e-04]],\n",
       "\n",
       "        [[4.4371e-04]],\n",
       "\n",
       "        [[2.9039e-04]],\n",
       "\n",
       "        [[2.7970e-04]],\n",
       "\n",
       "        [[3.3898e-04]],\n",
       "\n",
       "        [[3.3784e-04]],\n",
       "\n",
       "        [[4.0894e-04]],\n",
       "\n",
       "        [[4.3664e-04]],\n",
       "\n",
       "        [[4.4219e-04]],\n",
       "\n",
       "        [[2.9098e-04]],\n",
       "\n",
       "        [[3.6885e-04]],\n",
       "\n",
       "        [[2.3485e-04]],\n",
       "\n",
       "        [[2.8562e-04]],\n",
       "\n",
       "        [[2.7229e-04]],\n",
       "\n",
       "        [[3.4142e-04]],\n",
       "\n",
       "        [[9.8182e-05]],\n",
       "\n",
       "        [[4.3720e-04]],\n",
       "\n",
       "        [[3.4343e-04]],\n",
       "\n",
       "        [[3.3708e-04]],\n",
       "\n",
       "        [[2.5448e-04]],\n",
       "\n",
       "        [[4.7543e-04]],\n",
       "\n",
       "        [[3.2778e-04]],\n",
       "\n",
       "        [[4.2785e-04]],\n",
       "\n",
       "        [[3.9049e-04]],\n",
       "\n",
       "        [[3.3560e-04]],\n",
       "\n",
       "        [[4.1627e-04]],\n",
       "\n",
       "        [[1.9340e-04]],\n",
       "\n",
       "        [[3.6553e-04]],\n",
       "\n",
       "        [[1.3619e-04]],\n",
       "\n",
       "        [[3.9539e-04]],\n",
       "\n",
       "        [[1.6606e-04]],\n",
       "\n",
       "        [[3.8498e-04]],\n",
       "\n",
       "        [[3.8387e-04]],\n",
       "\n",
       "        [[3.4144e-04]],\n",
       "\n",
       "        [[3.0454e-04]],\n",
       "\n",
       "        [[1.7865e-04]],\n",
       "\n",
       "        [[4.1764e-04]],\n",
       "\n",
       "        [[3.5131e-04]],\n",
       "\n",
       "        [[3.2470e-04]],\n",
       "\n",
       "        [[3.3232e-04]],\n",
       "\n",
       "        [[4.1610e-04]],\n",
       "\n",
       "        [[4.4046e-04]],\n",
       "\n",
       "        [[4.9559e-04]],\n",
       "\n",
       "        [[3.4249e-04]],\n",
       "\n",
       "        [[4.8558e-04]],\n",
       "\n",
       "        [[3.1747e-04]],\n",
       "\n",
       "        [[2.0363e-04]],\n",
       "\n",
       "        [[1.1210e-04]],\n",
       "\n",
       "        [[3.3402e-04]],\n",
       "\n",
       "        [[3.5613e-04]],\n",
       "\n",
       "        [[3.2615e-04]],\n",
       "\n",
       "        [[3.5053e-04]],\n",
       "\n",
       "        [[3.3156e-04]],\n",
       "\n",
       "        [[2.7366e-04]],\n",
       "\n",
       "        [[2.7389e-04]],\n",
       "\n",
       "        [[5.4496e-04]],\n",
       "\n",
       "        [[3.8101e-04]],\n",
       "\n",
       "        [[3.9146e-04]],\n",
       "\n",
       "        [[4.7084e-04]],\n",
       "\n",
       "        [[3.8675e-04]],\n",
       "\n",
       "        [[2.8862e-04]],\n",
       "\n",
       "        [[2.7858e-04]],\n",
       "\n",
       "        [[2.5337e-04]],\n",
       "\n",
       "        [[2.8801e-04]],\n",
       "\n",
       "        [[3.1430e-04]],\n",
       "\n",
       "        [[3.3035e-04]],\n",
       "\n",
       "        [[2.1333e-04]],\n",
       "\n",
       "        [[2.1552e-04]],\n",
       "\n",
       "        [[3.0817e-04]],\n",
       "\n",
       "        [[3.1111e-04]],\n",
       "\n",
       "        [[4.0362e-04]],\n",
       "\n",
       "        [[3.1538e-04]],\n",
       "\n",
       "        [[2.2140e-04]],\n",
       "\n",
       "        [[4.4878e-04]],\n",
       "\n",
       "        [[3.9241e-04]],\n",
       "\n",
       "        [[4.1255e-04]],\n",
       "\n",
       "        [[3.6404e-04]],\n",
       "\n",
       "        [[3.1125e-04]],\n",
       "\n",
       "        [[9.9982e-01]],\n",
       "\n",
       "        [[9.9966e-01]],\n",
       "\n",
       "        [[9.9997e-01]],\n",
       "\n",
       "        [[9.9981e-01]],\n",
       "\n",
       "        [[9.9986e-01]],\n",
       "\n",
       "        [[9.9996e-01]],\n",
       "\n",
       "        [[9.9974e-01]],\n",
       "\n",
       "        [[9.9976e-01]],\n",
       "\n",
       "        [[9.9969e-01]],\n",
       "\n",
       "        [[9.9975e-01]],\n",
       "\n",
       "        [[9.9980e-01]],\n",
       "\n",
       "        [[9.9996e-01]],\n",
       "\n",
       "        [[9.9964e-01]],\n",
       "\n",
       "        [[9.9980e-01]],\n",
       "\n",
       "        [[9.9977e-01]],\n",
       "\n",
       "        [[9.9972e-01]],\n",
       "\n",
       "        [[9.9986e-01]],\n",
       "\n",
       "        [[9.9977e-01]],\n",
       "\n",
       "        [[9.9986e-01]],\n",
       "\n",
       "        [[9.9979e-01]],\n",
       "\n",
       "        [[9.9967e-01]],\n",
       "\n",
       "        [[9.9986e-01]],\n",
       "\n",
       "        [[9.9976e-01]],\n",
       "\n",
       "        [[9.9993e-01]],\n",
       "\n",
       "        [[9.9980e-01]],\n",
       "\n",
       "        [[9.9974e-01]],\n",
       "\n",
       "        [[9.9982e-01]],\n",
       "\n",
       "        [[9.9966e-01]],\n",
       "\n",
       "        [[9.9984e-01]],\n",
       "\n",
       "        [[9.9980e-01]],\n",
       "\n",
       "        [[9.9977e-01]],\n",
       "\n",
       "        [[9.9991e-01]],\n",
       "\n",
       "        [[9.9992e-01]],\n",
       "\n",
       "        [[9.9991e-01]],\n",
       "\n",
       "        [[9.9991e-01]],\n",
       "\n",
       "        [[9.9984e-01]],\n",
       "\n",
       "        [[9.9984e-01]],\n",
       "\n",
       "        [[9.9995e-01]],\n",
       "\n",
       "        [[9.9991e-01]],\n",
       "\n",
       "        [[9.9975e-01]],\n",
       "\n",
       "        [[9.9986e-01]],\n",
       "\n",
       "        [[9.9991e-01]],\n",
       "\n",
       "        [[9.9980e-01]],\n",
       "\n",
       "        [[9.9991e-01]],\n",
       "\n",
       "        [[9.9988e-01]],\n",
       "\n",
       "        [[9.9984e-01]],\n",
       "\n",
       "        [[9.9980e-01]],\n",
       "\n",
       "        [[9.9971e-01]],\n",
       "\n",
       "        [[9.9974e-01]],\n",
       "\n",
       "        [[9.9988e-01]],\n",
       "\n",
       "        [[9.9969e-01]],\n",
       "\n",
       "        [[9.9976e-01]],\n",
       "\n",
       "        [[9.9980e-01]],\n",
       "\n",
       "        [[9.9990e-01]],\n",
       "\n",
       "        [[9.9990e-01]],\n",
       "\n",
       "        [[9.9987e-01]],\n",
       "\n",
       "        [[9.9988e-01]],\n",
       "\n",
       "        [[9.9986e-01]],\n",
       "\n",
       "        [[9.9975e-01]],\n",
       "\n",
       "        [[9.9986e-01]],\n",
       "\n",
       "        [[9.9977e-01]],\n",
       "\n",
       "        [[9.9979e-01]],\n",
       "\n",
       "        [[9.9986e-01]],\n",
       "\n",
       "        [[9.9974e-01]],\n",
       "\n",
       "        [[9.9974e-01]],\n",
       "\n",
       "        [[9.9979e-01]],\n",
       "\n",
       "        [[9.9989e-01]],\n",
       "\n",
       "        [[9.9976e-01]],\n",
       "\n",
       "        [[9.9997e-01]],\n",
       "\n",
       "        [[9.9976e-01]],\n",
       "\n",
       "        [[9.9986e-01]],\n",
       "\n",
       "        [[9.9985e-01]],\n",
       "\n",
       "        [[9.9989e-01]],\n",
       "\n",
       "        [[9.9976e-01]],\n",
       "\n",
       "        [[9.9981e-01]],\n",
       "\n",
       "        [[9.9981e-01]],\n",
       "\n",
       "        [[9.9994e-01]],\n",
       "\n",
       "        [[9.9978e-01]],\n",
       "\n",
       "        [[9.9972e-01]],\n",
       "\n",
       "        [[9.9982e-01]],\n",
       "\n",
       "        [[9.9968e-01]],\n",
       "\n",
       "        [[9.9987e-01]],\n",
       "\n",
       "        [[9.9988e-01]],\n",
       "\n",
       "        [[9.9978e-01]],\n",
       "\n",
       "        [[9.9984e-01]],\n",
       "\n",
       "        [[9.9979e-01]],\n",
       "\n",
       "        [[9.9994e-01]],\n",
       "\n",
       "        [[9.9963e-01]],\n",
       "\n",
       "        [[9.9978e-01]],\n",
       "\n",
       "        [[9.9981e-01]],\n",
       "\n",
       "        [[9.9974e-01]],\n",
       "\n",
       "        [[9.9970e-01]],\n",
       "\n",
       "        [[9.9988e-01]],\n",
       "\n",
       "        [[9.9978e-01]],\n",
       "\n",
       "        [[9.9975e-01]],\n",
       "\n",
       "        [[9.9974e-01]],\n",
       "\n",
       "        [[9.9967e-01]],\n",
       "\n",
       "        [[9.9979e-01]],\n",
       "\n",
       "        [[9.9982e-01]],\n",
       "\n",
       "        [[9.9987e-01]],\n",
       "\n",
       "        [[9.9975e-01]],\n",
       "\n",
       "        [[9.9988e-01]],\n",
       "\n",
       "        [[9.9998e-01]],\n",
       "\n",
       "        [[9.9993e-01]],\n",
       "\n",
       "        [[9.9978e-01]],\n",
       "\n",
       "        [[9.9966e-01]],\n",
       "\n",
       "        [[9.9970e-01]],\n",
       "\n",
       "        [[9.9972e-01]],\n",
       "\n",
       "        [[9.9973e-01]],\n",
       "\n",
       "        [[9.9978e-01]],\n",
       "\n",
       "        [[9.9978e-01]],\n",
       "\n",
       "        [[9.9985e-01]],\n",
       "\n",
       "        [[9.9982e-01]],\n",
       "\n",
       "        [[9.9993e-01]],\n",
       "\n",
       "        [[9.9978e-01]],\n",
       "\n",
       "        [[9.9966e-01]],\n",
       "\n",
       "        [[9.9971e-01]],\n",
       "\n",
       "        [[9.9980e-01]],\n",
       "\n",
       "        [[9.9989e-01]],\n",
       "\n",
       "        [[9.9986e-01]],\n",
       "\n",
       "        [[9.9977e-01]],\n",
       "\n",
       "        [[9.9971e-01]],\n",
       "\n",
       "        [[9.9989e-01]],\n",
       "\n",
       "        [[9.9976e-01]],\n",
       "\n",
       "        [[9.9981e-01]],\n",
       "\n",
       "        [[9.9970e-01]],\n",
       "\n",
       "        [[9.9973e-01]],\n",
       "\n",
       "        [[9.9977e-01]],\n",
       "\n",
       "        [[9.9978e-01]],\n",
       "\n",
       "        [[9.9998e-01]],\n",
       "\n",
       "        [[9.9990e-01]],\n",
       "\n",
       "        [[9.9979e-01]],\n",
       "\n",
       "        [[9.9978e-01]],\n",
       "\n",
       "        [[9.9974e-01]],\n",
       "\n",
       "        [[9.9979e-01]],\n",
       "\n",
       "        [[9.9980e-01]],\n",
       "\n",
       "        [[9.9996e-01]],\n",
       "\n",
       "        [[9.9976e-01]],\n",
       "\n",
       "        [[9.9977e-01]],\n",
       "\n",
       "        [[9.9995e-01]],\n",
       "\n",
       "        [[9.9982e-01]],\n",
       "\n",
       "        [[9.9979e-01]],\n",
       "\n",
       "        [[9.9977e-01]]], device='cuda:0', grad_fn=<SigmoidBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(data_torch.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
